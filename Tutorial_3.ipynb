{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science at UCSB\n",
    "\n",
    "# Python for Data Science: ML Crash Course\n",
    "\n",
    "## Jason Freeberg, Fall 2016 \n",
    "\n",
    "Okay! Today will be a crash course in machine learning. I'll explain things at a high level and use Sci Kit Learn to show real examples. \"Feature engineering\" is the creation or collection of predictors for a machine learning pipeline, so we're covering ML first.\n",
    "\n",
    "To make an oversimplification, let's assume we have some set of *p* predictors, **X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub> ... X<sub>p</sub> ** for *n* observations. Then we have a corresponding set of dependent *n* variables, **Y**. **X** could be a set of 100 people (n=100), each with 10 variables (p=10) like height, weight, sex, location, education level, and so on. And, in the same example, **Y** could be that person's salary. What I just described is a *regression* problem. Where we have **X** and **Y**, and **Y** is a continuous variable. Now, from **X** and **Y** we can *learn* **F**, the mapping from **X** to **Y**. We can express it as **F**(**X**) = **Y** or in matrix notation as...\n",
    "\n",
    "$$ F \\left(\n",
    "\\begin{matrix}\n",
    "X_{1,1} & ... & X_{1,p} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "X_{n,1} & ... & X_{n,p} \\\\\n",
    "\\end{matrix}\n",
    "\\right) \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "Y_1 \\\\\n",
    "\\vdots \\\\\n",
    "Y_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There are two main branches of machine learning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "Like the example above, supervised learning involves using a matrix of *n*X*p* inputs, **X**, and *n* crorresponding outputs, **Y**, to build a statistical model that can then give predicted outputs from new, unseen inputs. As you might expect, this type of learning has broad applications to business, healthcare, and physics.\n",
    "\n",
    "In these problems, we assume that there is a true, unknown function of the form below, where espison is an irriducible error term with variance zero. We are attempting to approximate **F** as closely as possible because that is the error that we can reduce.\n",
    "\n",
    "$$ Y = f(X) + \\epsilon $$\n",
    "$$ ... $$\n",
    "$$ \n",
    "\\begin{align}\n",
    " E(Y - \\hat Y)^2 & = E(f(X) + \\epsilon - \\hat f(X))^2 \\\\\n",
    "& = (f(X) - \\hat f(X))^2  + Var(\\epsilon) \\\\\n",
    " \\end{align}$$\n",
    "\n",
    "\n",
    "#### Sci Kit Lean Example\n",
    "Similar to the problem above, imagine we have both **X** and **Y** and try to learn the mapping between them. But what if our dependant variable, **Y**, doesn't span the real numbers? There are many casses where we're trying to *classify* our outcomes... good or bad, alive or dead, pay or default... and these are **quantitative** or **classification** problems. \n",
    "\n",
    "Moreover, we can have multiple classes in **Y**. Think of tax brackets, image recognition, or types of crime. Food for thought: we can turn a regression problem into a classification problem simply by *binning* our outcomes. Salary in dollars would become income brackets. Then we could use classification algorithms instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows = 150 \n",
      " N cols = 5\n",
      "Classes in the dependent var. = {'Iris-virginica', 'Iris-versicolor', 'Iris-setosa'}\n",
      "   sepalLength  sepalWidth  petalLength  petalWidth        class\n",
      "0          5.1         3.5          1.4         0.2  Iris-setosa\n",
      "1          4.9         3.0          1.4         0.2  Iris-setosa\n",
      "2          4.7         3.2          1.3         0.2  Iris-setosa\n",
      "3          4.6         3.1          1.5         0.2  Iris-setosa\n",
      "4          5.0         3.6          1.4         0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "# Iris is a classic dataset. It holds various measurements of flowers and their species.\n",
    "# If we want to predict species from the measurements, what kind of problem are we \n",
    "# working with?\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import cross_validation, metrics\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "\n",
    "\n",
    "def read_csv_from_url(URL, columnNames):\n",
    "    response = urlopen(URL)\n",
    "    lines = pd.read_csv(response,\n",
    "                        header=None,\n",
    "                        index_col=False)\n",
    "    dataframe = pd.DataFrame(lines)\n",
    "    dataframe.columns = columnNames\n",
    "    return dataframe\n",
    "\n",
    "# Seeds make our random methods reproducible.\n",
    "seed = 123\n",
    "\n",
    "# Load the Iris dataset\n",
    "irisURL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "irisNames = ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'class']\n",
    "iris = read_csv_from_url(irisURL, irisNames)\n",
    "print('N rows =', iris.shape[0], '\\n', 'N cols =', iris.shape[1])\n",
    "print('Classes in the dependent var. =', set(iris['class']))\n",
    "print(iris.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Test Sets\n",
    "\n",
    "When you're studying for an exam at school you have your notes, review packets, and maybe some recorded lectures. You study, or *train*, with these materials and then you take the exam to see how well you understand the material you studied. The test has questions *similar* to what you studied, but not equal. So if you studied hard, you should be able to **generalize** well to these new \"inputs\".\n",
    "\n",
    "Likewise in machine learning, we need some data with the recorded inputs *and* outputs. Then when a model is trained on that data, we need to evaulate the model's perfomance by giving it new, unseen data. This is a common paradigm in machine and statistical learning, but it can be easy to mess up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: \n",
      " [[ 5.8  2.8  5.1  2.4]\n",
      " [ 6.3  3.4  5.6  2.4]\n",
      " [ 5.5  2.3  4.   1.3]\n",
      " [ 5.1  3.8  1.5  0.3]\n",
      " [ 4.4  3.   1.3  0.2]]\n",
      "Dependent variable: \n",
      " ['Iris-virginica' 'Iris-virginica' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "train, test = cross_validation.train_test_split(iris,\n",
    "                                                test_size=0.3,\n",
    "                                               random_state=seed)\n",
    "\n",
    "# Coerce the independent and dependent variable of the training set to NumPy arrays.\n",
    "predictors = np.array(train.ix[:, 0:-1])\n",
    "variable = np.array(train.ix[:, -1])\n",
    "\n",
    "print('Predictors:', '\\n', predictors[:5])\n",
    "print('Dependent variable:', '\\n', variable[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Algorithm\n",
    "\n",
    "Since Iris is a classic dataset, we're going to use a classic classification algorithm. You'll probably see this example a lot in books and online, it's *classifying the Iris dataset with **k nearest neighbors***. In kNN, we put all of our predictors in a *feature space*, stored with their classifications. Then when a new observation needs to be classified, we compare it to the nearest **k** observations and give it the classification of the majority nearest **k** observations.\n",
    "\n",
    "In the diagram below, you can see that the predicted class for the star can change depending on the number of neighbors used.\n",
    "\n",
    "![image](http://bdewilde.github.io/assets/images/2012-10-26-knn-concept.png)\n",
    "\n",
    "*Food for thought*: kNN can also be used for regression problems! In a regression context, we predict based on the mean target of the k neighbors. For a more detailed explanation on kNN classification, [click here](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/). Now for the code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Confusion Matrix ------------------\n",
      "[[18  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  0 17]]\n",
      "-------------- Incorrect Classifications --------------\n",
      "             class      prediction\n",
      "0  Iris-versicolor  Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "# KNN object and fit\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X=predictors,\n",
    "        y=variable)\n",
    "\n",
    "# Make predictions on test set\n",
    "testPredictors = test.ix[:, :-1]\n",
    "actual = test.ix[:, -1]\n",
    "predictions = knn.predict(testPredictors)\n",
    "\n",
    "# Merge the predicted and actual classifications\n",
    "predictionsDF = pd.DataFrame(predictions, columns=['prediction'])\n",
    "results = pd.concat([test.reset_index(drop=True), predictionsDF], axis=1)\n",
    "\n",
    "# Model evaluation\n",
    "matrix = metrics.confusion_matrix(y_true=results['class'],\n",
    "                                  y_pred=results['prediction'])\n",
    "\n",
    "print('------------------ Confusion Matrix ------------------')\n",
    "print(matrix)\n",
    "\n",
    "incorrect = results.ix[results['class'] != results['prediction'], ['class', 'prediction']]\n",
    "if incorrect.shape[0] == 0:\n",
    "    print('No incorrect classifications!')\n",
    "else:\n",
    "    print('-------------- Incorrect Classifications --------------')\n",
    "    print(incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "Supervised learning is fairly straightforward, as we saw in the example. However, unsupervised learning requires some abstraction. Essentially, in an unsupervised exercise we are trying to either uncover hidden structure, find similarities, or reduce dimensionality in the data.\n",
    "\n",
    "A common unsupervised example is clustering. If I were to hand you a bucket of rocks and ask you to put them into groups you may look at features like weight, volume, color and texture. Then you can group them by those characteristics. \n",
    "\n",
    "In a machine learning context, we can use unsupervised clustering as a pre-processing step. After our observations are clustered, we can easily add their cluster numbers as a column of predictors in the training data. Now we can use that new variable as a predictor in a regression or classification problem!\n",
    "\n",
    "Eventually you may come across the problem of having *too many predictors*. If you have a set of 10 million observations and 200 predictors, then building a model on all predictors will be very expensive and time consuming. Thankfully, using techniques like [Principle Component Analysis](http://colah.github.io/posts/2014-10-Visualizing-MNIST/), we can reduce the number of predictors to only those with a *high variability* and save time in the long run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction using PCA\n",
    "\n",
    "We're going to exemplify unsupervised learning with another classic dataset: the MNIST collection of handwritten digits. Each digit is a 28x28 pixel image, for 784 total pixels... \n",
    "\n",
    "\\begin{matrix}\n",
    "000 & 001 & 002 & 003 & ... & 026 & 027 \\\\\n",
    "028 & 029 & 030 & 031 & ... & 054 & 055 \\\\\n",
    "056 & 057 & 058 & 059 & ... & 082 & 083 \\\\\n",
    " \\vdots &  \\vdots &  \\vdots &  \\vdots & ... & \\vdots & \\vdots \\\\\n",
    "728 & 729 & 730 & 731 & ... & 754 & 755 \\\\\n",
    "756 & 757 & 758 & 759 & ... & 782 & 783 \\\\\n",
    "\\end{matrix}\n",
    "\n",
    "The .csv contains the level of darkness for each pixel, organized as a table with 785 columns and 42,000 rows. This translates to the 784 pixel values *plus* the actual digit labels, and 42,000 digits...\n",
    "\n",
    "\\begin{matrix}\n",
    "'1' & 001 & 002 & 003 & 004 & ... & 783 & 784 \\\\\n",
    "'7' & 001 & 002 & 003 & 004 & ... & 783 & 784 \\\\\n",
    "'3' & 001 & 002 & 003 & 004 & ... & 783 & 784 \\\\\n",
    " \\vdots & \\vdots &  \\vdots &  \\vdots &  \\vdots & ... & \\vdots & \\vdots \\\\\n",
    "'8' & 001 & 002 & 003 & 004 & ... & 783 & 784 \\\\\n",
    "'6' & 001 & 002 & 003 & 004 & ... & 783 & 784 \\\\\n",
    "\\end{matrix}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows = 42000 \n",
      " N cols = 785\n",
      "------------------ Head of Data ------------------\n",
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read data from the repo you downloaded\n",
    "location = os.path.realpath(os.path.join(os.getcwd(), \"digits.csv\"))\n",
    "digits = pd.read_csv(location)\n",
    "\n",
    "# Let's peak at the data\n",
    "print(\"N rows =\", digits.shape[0], '\\n', 'N cols =', digits.shape[1])\n",
    "print('------------------ Head of Data ------------------')\n",
    "print(digits.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, n_components=10, whiten=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "trainDigits, testDigits = cross_validation.train_test_split(digits,\n",
    "                                                            test_size=0.3,\n",
    "                                                            random_state=seed)\n",
    "# Coerce training data to np arrays\n",
    "trainLabel = trainDigits['label']\n",
    "trainPixels = trainDigits.ix[:, 1:]\n",
    "\n",
    "# Coerce testing data to np arrays\n",
    "testLabel = testDigits['label']\n",
    "testPixels = testDigits.ix[:, 1:]\n",
    "\n",
    "# Fit a PCA model\n",
    "pca = PCA(n_components = 10)\n",
    "pca.fit(X=trainPixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d6aab2e02189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's train a KMeans model with the 10 principle components.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# First we need to transform the training set using the PCA parameters...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainPCAPixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainPixels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPCAdigitsKNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pca' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's train a KMeans model with the 10 principle components.\n",
    "# First we need to transform the training set using the PCA parameters...\n",
    "trainPCAPixels = pca.transform(trainPixels)\n",
    "\n",
    "PCAdigitsKNN = KNeighborsClassifier()\n",
    "PCAdigitsKNN.fit(X=trainPCAPixels,\n",
    "              y=trainLabel)\n",
    "\n",
    "# And now make predictions on the test set.\n",
    "# First we need to transform the test set using the PCA parameters...\n",
    "testPCAPixels = pca.transform(testPixels)\n",
    "\n",
    "PCAdigitPredictions = PCAdigitsKNN.predict(testPCAPixels)\n",
    "PCAdigitPredictionsDF = pd.DataFrame(PCAdigitPredictions, columns=['prediction'])\n",
    "PCAdigitComparison = pd.concat([testLabel.reset_index(drop=True), PCAdigitPredictionsDF], axis=1)\n",
    "PCAdigitsIncorrect = results.ix[PCAdigitComparison['label'] != \\\n",
    "                             PCAdigitComparison['prediction'], ['class', 'prediction']].shape[0]\n",
    "\n",
    "PCAdigitsCorrect = PCAdigitComparison.shape[0] - PCAdigitsIncorrect\n",
    "print('Digits correctly classified =', PCAdigitsCorrect )\n",
    "print('Digits incorrectly classified =', PCAdigitsIncorrect)\n",
    "print('Raw accuracy =', round(PCAdigitsCorrect / float(PCAdigitsCorrect + PCAdigitsIncorrect), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our accuracy\\* of 99.98% to a baseline kNN using the full set of 784 features. Since we have over 70 times the predictors, the computation takes quite a while. For that reason, I downsampled the training data. If you want to see a *true* comparison, then comment out the sampling and *get ready to wait.*\n",
    "\n",
    "\\*Accuracy is a poor metric to evaluate classifiers. If we're classifying people as sick or healthy, and 99% of our population is healthy... then we could achieve 99% accuracy by simply saying that every person will be healthy! Check out [sensitivity and specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) for *binary* classifiers and [area under the curve](http://gim.unmc.edu/dxtests/roc3.htm) for *all* classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell takes a while to run. If you want to see the results, delete the block comment lines (\"\"\").\n",
    "\n",
    "\"\"\"\n",
    "digits2 = trainDigits.sample(frac=0.5,\n",
    "                                random_state=seed)\n",
    "\n",
    "trainDigits2, testDigits2= cross_validation.train_test_split(digits2,\n",
    "                                                            test_size=0.3,\n",
    "                                                            random_state=seed)\n",
    "\n",
    "# Get the pixels and labels from train data\n",
    "trainLabels2 = trainDigits2['label']\n",
    "trainPixels2 = trainDigits2.ix[:, 1:]\n",
    "\n",
    "# And the same from the testing data\n",
    "testLabels2 = testDigits2['label']\n",
    "testPixels2 = testDigits2.ix[:, 1:]\n",
    "\n",
    "digitsKNN = KNeighborsClassifier()\n",
    "digitsKNN.fit(X=trainPixels2,\n",
    "             y=trainLabels2)\n",
    "\n",
    "digitPredictions = digitsKNN.predict(testPixels2)\n",
    "\n",
    "digitPredictionsDF = pd.DataFrame(digitPredictions, columns=['prediction'])\n",
    "digitComparison = pd.concat([testLabels2.reset_index(drop=True), digitPredictionsDF], axis=1)\n",
    "digitsIncorrect = results.ix[digitComparison['label'] != \\\n",
    "                             digitComparison['prediction'], ['class', 'prediction']].shape[0]\n",
    "\n",
    "digitsCorrect = digitComparison.shape[0] - digitsIncorrect\n",
    "print 'Digits correctly classified =', digitsCorrect \n",
    "print 'Digits incorrectly classified =', digitsIncorrect\n",
    "print 'Raw accuracy =', round(digitsCorrect / float(digitsCorrect + digitsIncorrect), 4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Your turn!\n",
    "\n",
    "Alrighty! With the class time left you're going to load **and clean** the UCI dataset containing hourly measurements of road-level air quality in a busy Italian city. Then build a multiple linear regression model to predict Carbon Monoxidet from the predictors. \n",
    "\n",
    "Consult the [SciKit Learn docs](http://scikit-learn.org/stable/modules/classes.html) and the earlier code to get help. You can also ask Jason, but give it a solid attempt before asking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f168b196469f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Then go back, uncomment the line to remove the empty columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlocation2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AirQualityUCI.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mairQuality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the dataset, and pring the head.\n",
    "# Then go back, uncomment the line to remove the empty columns\n",
    "\n",
    "location2 = os.path.realpath(os.path.join(os.getcwd(), \"AirQualityUCI.csv\"))\n",
    "airQuality = pd.read_csv(location2, delimiter = ';', decimal = ',')\n",
    "\n",
    "airQuality.drop(airQuality[[15,16]], axis=1, inplace=True)\n",
    "\n",
    "print(airQuality.head(10))\n",
    "\n",
    "assert airQuality.shape[1] == 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'airQuality' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b71b469ed730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Hint for the first <FILL IN>: the values can happen in any row or column.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mairQuality\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mairQuality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'N rows ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mairQuality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'N cols ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mairQuality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'airQuality' is not defined"
     ]
    }
   ],
   "source": [
    "# The air sensor sometime failed, and mising values were replaced with -200. So let's replace\n",
    "# that with NumPy's NaN value.\n",
    "\n",
    "# Hint for the first <FILL IN>: the values can happen in any row or column.\n",
    "airQuality[airQuality.ix[:,:] == -200] = np.nan\n",
    "\n",
    "print('N rows =', airQuality.shape[0], '\\n', 'N cols =', airQuality.shape[1])\n",
    "print(airQuality.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6941, 14)\n",
      "         Date      Time  CO(GT)  PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)  \\\n",
      "0  10/03/2004  18.00.00     2.6       1360.0      11.9         1046.0   \n",
      "1  10/03/2004  19.00.00     2.0       1292.0       9.4          955.0   \n",
      "2  10/03/2004  20.00.00     2.2       1402.0       9.0          939.0   \n",
      "3  10/03/2004  21.00.00     2.2       1376.0       9.2          948.0   \n",
      "4  10/03/2004  22.00.00     1.6       1272.0       6.5          836.0   \n",
      "\n",
      "   NOx(GT)  PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)     T    RH  \\\n",
      "0    166.0        1056.0    113.0        1692.0       1268.0  13.6  48.9   \n",
      "1    103.0        1174.0     92.0        1559.0        972.0  13.3  47.7   \n",
      "2    131.0        1140.0    114.0        1555.0       1074.0  11.9  54.0   \n",
      "3    172.0        1092.0    122.0        1584.0       1203.0  11.0  60.0   \n",
      "4    131.0        1205.0    116.0        1490.0       1110.0  11.2  59.6   \n",
      "\n",
      "       AH  \n",
      "0  0.7578  \n",
      "1  0.7255  \n",
      "2  0.7502  \n",
      "3  0.7867  \n",
      "4  0.7888  \n",
      "Date             0\n",
      "Time             0\n",
      "CO(GT)           0\n",
      "PT08.S1(CO)      0\n",
      "C6H6(GT)         0\n",
      "PT08.S2(NMHC)    0\n",
      "NOx(GT)          0\n",
      "PT08.S3(NOx)     0\n",
      "NO2(GT)          0\n",
      "PT08.S4(NO2)     0\n",
      "PT08.S5(O3)      0\n",
      "T                0\n",
      "RH               0\n",
      "AH               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Holy shit! That hyrdocarbon sensor really sucks! \n",
    "# For the sake of time, let's just drop that column too and remove rows with any NaNs.\n",
    "\n",
    "airQuality.drop('NMHC(GT)', axis=1, inplace=True)\n",
    "airQuality.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "print(airQuality.shape)\n",
    "print(airQuality.head())\n",
    "print(airQuality.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  NO2(GT)  \\\n",
      "6968       1053.0       3.5          680.0    166.0         890.0    108.0   \n",
      "229        1064.0       4.3          728.0    115.0        1161.0     91.0   \n",
      "6645       1250.0      14.5         1133.0    508.0         629.0    168.0   \n",
      "9199        963.0       2.9          646.0    117.0         834.0     83.0   \n",
      "221        1215.0       8.3          912.0    127.0         948.0    109.0   \n",
      "\n",
      "      PT08.S4(NO2)  PT08.S5(O3)     T    RH      AH  \n",
      "6968        1166.0        807.0  11.3  80.1  1.0741  \n",
      "229         1461.0        842.0  13.5  58.4  0.8960  \n",
      "6645        1274.0       1463.0  17.5  30.8  0.6095  \n",
      "9199        1181.0        846.0  14.7  66.4  1.1006  \n",
      "221         1547.0        993.0  14.2  58.3  0.9380  \n",
      "------------------------------------------\n",
      "      PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  NO2(GT)  \\\n",
      "7144       1016.0       4.9          760.0    254.0         835.0    112.0   \n",
      "7236        939.0       1.8          568.0    101.0        1037.0     72.0   \n",
      "4668       1013.0       8.1          905.0    231.0         804.0     79.0   \n",
      "8401       1194.0      11.7         1038.0    388.0         605.0    223.0   \n",
      "1979        854.0       2.4          613.0     39.0        1272.0     41.0   \n",
      "\n",
      "      PT08.S4(NO2)  PT08.S5(O3)     T    RH      AH  \n",
      "7144        1044.0       1016.0   4.1  65.8  0.5451  \n",
      "7236        1054.0        656.0  12.4  62.6  0.8994  \n",
      "4668        1499.0        974.0  18.3  65.5  1.3686  \n",
      "8401        1317.0       1181.0   4.8  78.6  0.6826  \n",
      "1979        1515.0        565.0  16.8  81.3  1.5452  \n"
     ]
    }
   ],
   "source": [
    "# Great, now our data is free of missing values and we still have 7,400 observations!\n",
    "# Now let's fit a model!\n",
    "\n",
    "airTrain, airTest = cross_validation.train_test_split(airQuality, \n",
    "                                                      test_size=0.3,\n",
    "                                                      random_state=seed)\n",
    "Ncolumns = airTrain.shape[1]\n",
    "# Pull out the labels from the train and test sets\n",
    "trainLabels = airTrain['CO(GT)']\n",
    "testLabels = airTest['CO(GT)']\n",
    "\n",
    "# Pull out the predictors from the train and test sets\n",
    "# Do NOT select 'Date' as a pedictor\n",
    "trainVariables = airTrain.drop(['Date', 'CO(GT)', 'Time'], axis = 1, inplace = False)\n",
    "testVariables = airTest.drop(['Date', 'CO(GT)', 'Time'], axis = 1, inplace = False)\n",
    "\n",
    "print(trainVariables.head())\n",
    "print('------------------------------------------')\n",
    "print(testVariables.head())\n",
    "\n",
    "assert trainVariables.shape[1] == testVariables.shape[1] == 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modeling time!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linearModel = LinearRegression()\n",
    "linearModel.fit(X = trainVariables,\n",
    "                y = trainLabels)\n",
    "\n",
    "# Now join the true and predicted values together\n",
    "testPredictions = linearModel.predict(testVariables)\n",
    "testPredictionsDF = pd.DataFrame(testPredictions, columns=['prediction'])\n",
    "testComparison = pd.concat([testLabels.reset_index(drop=True), testPredictionsDF], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well we did! Since this was a regression problem, we'll use the Mean Squared Error (MSE) to evaluate the model.\n",
    "\n",
    "$$ \\begin{align}\n",
    " MSE & = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat Y_i)^2 \\\\\n",
    " & = \\frac{1}{n} \\sum_{i=1}^n (f(y_i) - \\hat f(y_i) )^2 \n",
    " \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.1715\n",
      "Root MSE = 0.4141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "# Let's see how well it did. The function below takes two columns, one of true values and \n",
    "# another with the predicted.\n",
    "\n",
    "error = mean_squared_error(testComparison[[0]], testComparison[[1]])\n",
    "print('MSE =', round(error, 4))\n",
    "print('Root MSE =', round(math.sqrt(error),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is in the units of the dependent variable, so now we can say... \"We expect our model to predict the concentration of Carbon Monoxide within 0.41 $ \\frac{mg}{m^3} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thanks to...\n",
    "\n",
    "- [The MNIST digits dataset from Kaggle](https://www.kaggle.com/c/digit-recognizer/data)\n",
    "- [The UCI Iris dataset](https://archive.ics.uci.edu/ml/datasets/Iris)\n",
    "- [The UCI Air Quality dataset](https://archive.ics.uci.edu/ml/datasets/Air+Quality)\n",
    "- [This **great** blog post on PCA](http://colah.github.io/posts/2014-10-Visualizing-MNIST/)\n",
    "- [This Git Blog](http://bdewilde.github.io/blog/blogger/2012/10/26/classification-of-hand-written-digits-3/) for the kNN illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
