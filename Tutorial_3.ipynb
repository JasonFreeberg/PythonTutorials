{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science at UCSB\n",
    "\n",
    "# Python for Data Science: ML Crash Course\n",
    "\n",
    "## Jason Freeberg, Fall 2016 \n",
    "\n",
    "Okay! Today will be a crash course in machine learning. I'll explain things at a high level and use Sci Kit Learn to show real examples. \"Feature engineering\" is the creation or collection of predictors for a machine learning pipeline, so we're covering ML first.\n",
    "\n",
    "To make an oversimplification, let's assume we have some set of *p* predictors, **X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub> ... X<sub>p</sub> ** for *n* observations. Assume we also have a corresponding set of *n* dependent variables, **Y**. Our matrix, **X**, could be a set of 100 people (n=100), each with 10 variables (p=10) like height, weight, sex, location, education level, and so on. And in the same example, **Y** could be that person's salary. What I just described is a *regression* problem. Where we have **X** and **Y**, and **Y** is a continuous variable. Now, from **X** and **Y** we can *learn* **F**, the mapping from **X** to **Y**. We can express it as **F**(**X**) = **Y** or in matrix notation as...\n",
    "\n",
    "$$ F \\left(\n",
    "\\begin{matrix}\n",
    "X_{1,1} & ... & X_{1,p} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "X_{n,1} & ... & X_{n,p} \\\\\n",
    "\\end{matrix}\n",
    "\\right) \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "Y_1 \\\\\n",
    "\\vdots \\\\\n",
    "Y_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There are two main branches of machine learning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "Like the example above, supervised learning involves using a matrix of *n*X*p* inputs, **X**, and *n* crorresponding outputs, **Y**, to build a statistical model that can then give predictions from new, unseen inputs. As you might expect, this type of learning has broad applications in business, healthcare, and physics.\n",
    "\n",
    "In these problems, we assume that there is a true, unknown function of the form below, where espison is an irriducible error term with variance zero. We are attempting to approximate **F** as closely as possible because we cannot reduce epsilon. Let $ E(Y - \\hat Y)^2 $ be the average squared difference between the prediction and the true output.\n",
    "\n",
    "$$ Y = f(X) + \\epsilon $$\n",
    "\n",
    "$$ ... $$\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    " E(Y - \\hat Y)^2 & = E(f(X) + \\epsilon - \\hat f(X))^2 \\\\\n",
    "& = (f(X) - \\hat f(X))^2  + Var(\\epsilon) \\\\\n",
    " \\end{align}$$\n",
    "\n",
    "We can see that minimizing the difference between the actual and the predicted output is where we can improve the model.\n",
    "\n",
    "#### Sci Kit Lean Example\n",
    "Similar to the problem above, imagine we have both **X** and **Y** and try to learn the mapping between them. But what if our dependant variable, **Y**, doesn't span the real numbers? There are many casses where we're trying to *classify* our outcomes... good or bad, alive or dead, pay or default... and these are **quantitative** or **classification** problems. \n",
    "\n",
    "Moreover, we can have multiple classes in **Y**. Think of tax brackets, image recognition, or types of crime. Food for thought: we can turn a regression problem into a classification problem simply by *binning* our outcomes. Salary in dollars would become income brackets. Then we could use classification algorithms instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows = 150\n",
      "Number of columns = 5\n",
      "Classes in the dependent variable = ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n",
      "   sepalLength  sepalWidth  petalLength  petalWidth        class\n",
      "0          5.1         3.5          1.4         0.2  Iris-setosa\n",
      "1          4.9         3.0          1.4         0.2  Iris-setosa\n",
      "2          4.7         3.2          1.3         0.2  Iris-setosa\n",
      "3          4.6         3.1          1.5         0.2  Iris-setosa\n",
      "4          5.0         3.6          1.4         0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "# Iris is a classic dataset. It holds various measurements of flowers and their species.\n",
    "# If we want to predict species from the measurements, what kind of problem are we \n",
    "# working with?\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import cross_validation, metrics\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "\n",
    "\n",
    "def read_csv_from_url(URL, columnNames):\n",
    "    response = urlopen(URL)\n",
    "    lines = pd.read_csv(response,\n",
    "                        header=None,\n",
    "                        index_col=False)\n",
    "    dataframe = pd.DataFrame(lines)\n",
    "    dataframe.columns = columnNames\n",
    "    return dataframe\n",
    "\n",
    "# Seeds make our random methods reproducible.\n",
    "seed = 123\n",
    "\n",
    "# Load the Iris dataset\n",
    "irisURL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "irisNames = ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'class']\n",
    "iris = read_csv_from_url(irisURL, irisNames)\n",
    "\n",
    "# Print some info on our data\n",
    "print('Number of rows =', iris.shape[0]) \n",
    "print('Number of columns =', iris.shape[1])\n",
    "print('Classes in the dependent variable =', (iris['class'].unique()))\n",
    "print(iris.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Test Sets\n",
    "\n",
    "When you're studying for an exam at school you have your notes, review packets, and maybe some recorded lectures. You study, or *train*, with these materials and then you take the exam to see how well you understand the material you studied. The test has questions *similar* to what you studied, but not equal. So if you studied hard, you should be able to **generalize** well to these new \"inputs\".\n",
    "\n",
    "Likewise in machine learning, we need some data with the recorded inputs *and* outputs. Then when a model is trained on that data, we need to evaulate the model's perfomance by giving it new, unseen data. This is a common paradigm in machine and statistical learning, but it can be easy to mess up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: \n",
      " [[ 5.8  2.8  5.1  2.4]\n",
      " [ 6.3  3.4  5.6  2.4]\n",
      " [ 5.5  2.3  4.   1.3]\n",
      " [ 5.1  3.8  1.5  0.3]\n",
      " [ 4.4  3.   1.3  0.2]]\n",
      "Dependent variable: \n",
      " ['Iris-virginica' 'Iris-virginica' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "train, test = cross_validation.train_test_split(iris,\n",
    "                                                test_size=0.3,\n",
    "                                               random_state=seed)\n",
    "\n",
    "# Coerce the independent and dependent variable of the training set to NumPy arrays.\n",
    "predictors = np.array(train.ix[:, 0:-1])\n",
    "variable = np.array(train.ix[:, -1])\n",
    "\n",
    "print('Predictors:', '\\n', predictors[:5])\n",
    "print('Dependent variable:', '\\n', variable[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Algorithm\n",
    "\n",
    "Since Iris is a classic dataset, we're going to use a classic classification algorithm. You'll probably see this example a lot in books and online, it's *classifying the Iris dataset with **k nearest neighbors***. In kNN, we put all of our predictors in a *feature space*, stored with their classifications. Then when a new observation needs to be classified, we compare it to the nearest **k** observations and give it the classification of the majority nearest **k** observations.\n",
    "\n",
    "In the diagram below, you can see that the predicted class for the star can change depending on the number of neighbors used.\n",
    "\n",
    "![image](http://bdewilde.github.io/assets/images/2012-10-26-knn-concept.png)\n",
    "\n",
    "*Food for thought*: kNN can also be used for regression problems! In a regression context, we predict based on the mean target of the k neighbors. For a more detailed explanation on kNN classification, [click here](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/). Now for the code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7b288e5514a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# KNN object and fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m knn.fit(X=predictors,\n\u001b[0m\u001b[1;32m      4\u001b[0m         y=variable)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictors' is not defined"
     ]
    }
   ],
   "source": [
    "# KNN object and fit\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X=predictors,\n",
    "        y=variable)\n",
    "\n",
    "# Make predictions on test set\n",
    "testPredictors = test.ix[:, :-1]\n",
    "actual = test.ix[:, -1]\n",
    "predictions = knn.predict(testPredictors)\n",
    "\n",
    "# Merge the predicted and actual classifications\n",
    "predictionsDF = pd.DataFrame(predictions, columns=['prediction'])\n",
    "results = pd.concat([test.reset_index(drop=True), predictionsDF], axis=1)\n",
    "\n",
    "# Model evaluation\n",
    "matrix = metrics.confusion_matrix(y_true=results['class'],\n",
    "                                  y_pred=results['prediction'])\n",
    "\n",
    "print('------------------ Confusion Matrix ------------------')\n",
    "print(matrix)\n",
    "\n",
    "incorrect = results.ix[results['class'] != results['prediction'], ['class', 'prediction']]\n",
    "\n",
    "if incorrect.shape[0] == 0:\n",
    "    print('No incorrect classifications!')\n",
    "else:\n",
    "    print('-------------- Incorrect Classifications --------------')\n",
    "    print(incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "Supervised learning is fairly straightforward, as we saw in the example. However, unsupervised learning requires some abstraction. Essentially, in an unsupervised exercise we are trying to either uncover hidden structure, find similarities, or reduce dimensionality in the data.\n",
    "\n",
    "A common unsupervised example is clustering. If I were to hand you a bucket of rocks and ask you to put them into groups you may look at features like weight, volume, color and texture. Then you can group them by those characteristics. \n",
    "\n",
    "In a machine learning context, we can use unsupervised clustering as a pre-processing step. After our observations are clustered, we can easily add their cluster numbers as a column of predictors in the training data. Now we can use that new variable as a predictor in a regression or classification problem!\n",
    "\n",
    "Eventually you may come across the problem of having *too many predictors*. If you have a set of 10 million observations and 200 predictors, then building a model on all predictors will be very expensive and time consuming. Thankfully, using techniques like [Principle Component Analysis](http://colah.github.io/posts/2014-10-Visualizing-MNIST/), we can reduce the number of predictors to only those with a *high variability* and save time in the long run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction using PCA\n",
    "\n",
    "We're going to work through an unsupervised learning example with another classic dataset: the MNIST collection of handwritten digits. Each digit is an image containing 28x28 pixels, for 784 total pixels laid out as below. Each pixel contains a number in the range from 0 to 1. 0 codes for a white pixel, 1 codes for a black pixel.\n",
    "\n",
    "\\begin{matrix}\n",
    "000 & 001 & 002 & 003 & ... & 026 & 027 \\\\\n",
    "028 & 029 & 030 & 031 & ... & 054 & 055 \\\\\n",
    "056 & 057 & 058 & 059 & ... & 082 & 083 \\\\\n",
    " \\vdots &  \\vdots &  \\vdots &  \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "728 & 729 & 730 & 731 & ... & 754 & 755 \\\\\n",
    "756 & 757 & 758 & 759 & ... & 782 & 783 \\\\\n",
    "\\end{matrix}\n",
    "\n",
    "The .csv contains this pixel data for 42,000 digits, organized as a table with 785 columns and 42,000 rows. This translates to the same 784 pixel values *plus* the actual digit labels. And with 42,000 digits we get a DataFrame laid out as so...\n",
    "\n",
    "\\begin{matrix}\n",
    "\"4\" & 000 & 001 & 002 & 003 & ... & 782 & 783 \\\\\n",
    "\"7\" & 000 & 001 & 002 & 003 & ... & 782 & 783 \\\\\n",
    "\"3\" & 000 & 001 & 002 & 003 & ... & 782 & 783 \\\\\n",
    " \\vdots & \\vdots &  \\vdots &  \\vdots &  \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "\"8\" & 000 & 001 & 002 & 003 & ... & 782 & 783 \\\\\n",
    "\"6\" & 000 & 001 & 002 & 003 & ... & 782 & 783 \\\\\n",
    "\\end{matrix}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows = 42000 \n",
      " N cols = 785\n",
      "------------------ Head of Data ------------------\n",
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read data from the repo you downloaded\n",
    "location = os.path.realpath(os.path.join(os.getcwd(), \"digits.csv\"))\n",
    "digits = pd.read_csv(location)\n",
    "\n",
    "# Let's peak at the data\n",
    "print(\"N rows =\", digits.shape[0], '\\n', 'N cols =', digits.shape[1])\n",
    "print('------------------ Head of Data ------------------')\n",
    "print(digits.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, n_components=10, whiten=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "trainDigits, testDigits = cross_validation.train_test_split(digits,\n",
    "                                                            test_size=0.3,\n",
    "                                                            random_state=seed)\n",
    "# Coerce training data to np arrays\n",
    "trainLabel = trainDigits['label']\n",
    "trainPixels = trainDigits.ix[:, 1:]\n",
    "\n",
    "# Coerce testing data to np arrays\n",
    "testLabel = testDigits['label']\n",
    "testPixels = testDigits.ix[:, 1:]\n",
    "\n",
    "# Fit a PCA model\n",
    "pca = PCA(n_components = 10)\n",
    "pca.fit(X=trainPixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d6aab2e02189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's train a KMeans model with the 10 principle components.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# First we need to transform the training set using the PCA parameters...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainPCAPixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainPixels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPCAdigitsKNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pca' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's train a kNN model with the 10 principle components.\n",
    "# First we need to transform the training set using the PCA parameters...\n",
    "trainPCAPixels = pca.transform(trainPixels)\n",
    "\n",
    "PCAdigitsKNN = KNeighborsClassifier()\n",
    "PCAdigitsKNN.fit(X=trainPCAPixels,\n",
    "              y=trainLabel)\n",
    "\n",
    "# And now make predictions on the test set.\n",
    "# First we need to transform the test set using the PCA parameters...\n",
    "testPCAPixels = pca.transform(testPixels)\n",
    "\n",
    "PCAdigitPredictions = PCAdigitsKNN.predict(testPCAPixels)\n",
    "PCAdigitPredictionsDF = pd.DataFrame(PCAdigitPredictions, columns=['prediction'])\n",
    "PCAdigitComparison = pd.concat([testLabel.reset_index(drop=True), PCAdigitPredictionsDF], axis=1)\n",
    "PCAdigitsIncorrect = results.ix[PCAdigitComparison['label'] != \\\n",
    "                             PCAdigitComparison['prediction'], ['class', 'prediction']].shape[0]\n",
    "\n",
    "PCAdigitsCorrect = PCAdigitComparison.shape[0] - PCAdigitsIncorrect\n",
    "print('Digits correctly classified =', PCAdigitsCorrect )\n",
    "print('Digits incorrectly classified =', PCAdigitsIncorrect)\n",
    "print('Raw accuracy =', round(PCAdigitsCorrect / float(PCAdigitsCorrect + PCAdigitsIncorrect), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our accuracy\\* of 99.98% to a baseline kNN using the full set of 784 features. Since we have over 70 times the predictors, the computation takes quite a while. For that reason, I downsampled the training data. If you want to see a *true* comparison, then comment out the sampling and **get ready to wait.**\n",
    "\n",
    "\\* *Accuracy is a poor metric to evaluate classifiers. Example: we are classifying people as sick or healthy, and 99% of our population is healthy... then we could achieve 99% accuracy by simply predicting that every person will be healthy! Check out [sensitivity and specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) and [area under the curve](http://gim.unmc.edu/dxtests/roc3.htm) as a better metric for classification.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell takes a while to run. If you want to see the results, delete the block comment lines (\"\"\").\n",
    "\n",
    "\"\"\"\n",
    "digits2 = trainDigits.sample(frac=0.5,\n",
    "                                random_state=seed)\n",
    "\n",
    "trainDigits2, testDigits2= cross_validation.train_test_split(digits2,\n",
    "                                                            test_size=0.3,\n",
    "                                                            random_state=seed)\n",
    "\n",
    "# Get the pixels and labels from train data\n",
    "trainLabels2 = trainDigits2['label']\n",
    "trainPixels2 = trainDigits2.ix[:, 1:]\n",
    "\n",
    "# And the same from the testing data\n",
    "testLabels2 = testDigits2['label']\n",
    "testPixels2 = testDigits2.ix[:, 1:]\n",
    "\n",
    "digitsKNN = KNeighborsClassifier()\n",
    "digitsKNN.fit(X=trainPixels2,\n",
    "             y=trainLabels2)\n",
    "\n",
    "digitPredictions = digitsKNN.predict(testPixels2)\n",
    "\n",
    "digitPredictionsDF = pd.DataFrame(digitPredictions, columns=['prediction'])\n",
    "digitComparison = pd.concat([testLabels2.reset_index(drop=True), digitPredictionsDF], axis=1)\n",
    "digitsIncorrect = results.ix[digitComparison['label'] != \\\n",
    "                             digitComparison['prediction'], ['class', 'prediction']].shape[0]\n",
    "\n",
    "digitsCorrect = digitComparison.shape[0] - digitsIncorrect\n",
    "print 'Digits correctly classified =', digitsCorrect \n",
    "print 'Digits incorrectly classified =', digitsIncorrect\n",
    "print 'Raw accuracy =', round(digitsCorrect / float(digitsCorrect + digitsIncorrect), 4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Your turn!\n",
    "\n",
    "Alrighty! With the class time left you're going to load **and clean** the UCI dataset containing hourly measurements of road-level air quality in a busy Italian city. Then build a multiple linear regression model to predict the concentration of Carbon Monoxide, 'CO(GT)', from the predictors. \n",
    "\n",
    "Consult the [SciKit Learn docs](http://scikit-learn.org/stable/modules/classes.html) and the earlier code to get help. You can also ask Jason, but give it a solid attempt before asking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f168b196469f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Then go back, uncomment the line to remove the empty columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlocation2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AirQualityUCI.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mairQuality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Run this cell to load the dataset and print the head.\n",
    "# Then go back, uncomment and complete the line to remove the empty columns\n",
    "\n",
    "location2 = os.path.realpath(os.path.join(os.getcwd(), \"AirQualityUCI.csv\"))\n",
    "airQuality = pd.read_csv(location2, delimiter = ';', decimal = ',')\n",
    "\n",
    "#airQuality.drop(airQuality[<FILL IN>], axis=1, inplace=True)\n",
    "\n",
    "print(airQuality.head(10))\n",
    "\n",
    "assert airQuality.shape[1] == 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'airQuality' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b71b469ed730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Hint for the first <FILL IN>: the values can happen in any row or column.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mairQuality\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mairQuality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'N rows ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mairQuality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'N cols ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mairQuality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'airQuality' is not defined"
     ]
    }
   ],
   "source": [
    "# The air sensor sometime failed, and mising values were replaced with -200. So let's replace\n",
    "# that with NumPy's NaN value.\n",
    "\n",
    "# Hint for the first <FILL IN>: the values can happen in any row or column.\n",
    "airQuality[airQuality.ix[<FILL IN>, <FILL IN>] == -200] = np.nan\n",
    "\n",
    "print('N rows =', airQuality.shape[0], '\\n', 'N cols =', airQuality.shape[1])\n",
    "print(airQuality.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6941, 14)\n",
      "         Date      Time  CO(GT)  PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)  \\\n",
      "0  10/03/2004  18.00.00     2.6       1360.0      11.9         1046.0   \n",
      "1  10/03/2004  19.00.00     2.0       1292.0       9.4          955.0   \n",
      "2  10/03/2004  20.00.00     2.2       1402.0       9.0          939.0   \n",
      "3  10/03/2004  21.00.00     2.2       1376.0       9.2          948.0   \n",
      "4  10/03/2004  22.00.00     1.6       1272.0       6.5          836.0   \n",
      "\n",
      "   NOx(GT)  PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)     T    RH  \\\n",
      "0    166.0        1056.0    113.0        1692.0       1268.0  13.6  48.9   \n",
      "1    103.0        1174.0     92.0        1559.0        972.0  13.3  47.7   \n",
      "2    131.0        1140.0    114.0        1555.0       1074.0  11.9  54.0   \n",
      "3    172.0        1092.0    122.0        1584.0       1203.0  11.0  60.0   \n",
      "4    131.0        1205.0    116.0        1490.0       1110.0  11.2  59.6   \n",
      "\n",
      "       AH  \n",
      "0  0.7578  \n",
      "1  0.7255  \n",
      "2  0.7502  \n",
      "3  0.7867  \n",
      "4  0.7888  \n",
      "Date             0\n",
      "Time             0\n",
      "CO(GT)           0\n",
      "PT08.S1(CO)      0\n",
      "C6H6(GT)         0\n",
      "PT08.S2(NMHC)    0\n",
      "NOx(GT)          0\n",
      "PT08.S3(NOx)     0\n",
      "NO2(GT)          0\n",
      "PT08.S4(NO2)     0\n",
      "PT08.S5(O3)      0\n",
      "T                0\n",
      "RH               0\n",
      "AH               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Holy shit! That hyrdocarbon sensor really sucks! \n",
    "# For the sake of time, let's just drop that column too and remove rows with any NaNs.\n",
    "\n",
    "airQuality.drop(<FILL IN>, axis=1, inplace=True)\n",
    "airQuality.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "print(airQuality.shape)\n",
    "print(airQuality.head())\n",
    "print(airQuality.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  NO2(GT)  \\\n",
      "6968       1053.0       3.5          680.0    166.0         890.0    108.0   \n",
      "229        1064.0       4.3          728.0    115.0        1161.0     91.0   \n",
      "6645       1250.0      14.5         1133.0    508.0         629.0    168.0   \n",
      "9199        963.0       2.9          646.0    117.0         834.0     83.0   \n",
      "221        1215.0       8.3          912.0    127.0         948.0    109.0   \n",
      "\n",
      "      PT08.S4(NO2)  PT08.S5(O3)     T    RH      AH  \n",
      "6968        1166.0        807.0  11.3  80.1  1.0741  \n",
      "229         1461.0        842.0  13.5  58.4  0.8960  \n",
      "6645        1274.0       1463.0  17.5  30.8  0.6095  \n",
      "9199        1181.0        846.0  14.7  66.4  1.1006  \n",
      "221         1547.0        993.0  14.2  58.3  0.9380  \n",
      "------------------------------------------\n",
      "      PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  NO2(GT)  \\\n",
      "7144       1016.0       4.9          760.0    254.0         835.0    112.0   \n",
      "7236        939.0       1.8          568.0    101.0        1037.0     72.0   \n",
      "4668       1013.0       8.1          905.0    231.0         804.0     79.0   \n",
      "8401       1194.0      11.7         1038.0    388.0         605.0    223.0   \n",
      "1979        854.0       2.4          613.0     39.0        1272.0     41.0   \n",
      "\n",
      "      PT08.S4(NO2)  PT08.S5(O3)     T    RH      AH  \n",
      "7144        1044.0       1016.0   4.1  65.8  0.5451  \n",
      "7236        1054.0        656.0  12.4  62.6  0.8994  \n",
      "4668        1499.0        974.0  18.3  65.5  1.3686  \n",
      "8401        1317.0       1181.0   4.8  78.6  0.6826  \n",
      "1979        1515.0        565.0  16.8  81.3  1.5452  \n"
     ]
    }
   ],
   "source": [
    "# Great, now our data is free of missing values and we still have 7,400 observations!\n",
    "# Now let's fit a model!\n",
    "\n",
    "# Split the train and test sets into TWO dataframes: airTrain, and airTest\n",
    "<FILL IN>, <FILL IN> = cross_validation.train_test_split(airQuality, \n",
    "                                                      test_size=0.3,\n",
    "                                                      random_state=seed)\n",
    "Ncolumns = airTrain.shape[1]\n",
    "\n",
    "# Pull out the labels from the train and test sets. (label == dependent variable)\n",
    "trainLabels = airTrain[<FILL IN>]\n",
    "testLabels = airTest[<FILL IN>]\n",
    "\n",
    "# Pull out the predictors from the train and test sets\n",
    "# Do NOT select 'Date' or 'Time' as pedictors\n",
    "trainVariables = airTrain.drop([<FILL IN>], axis = 1, inplace = False)\n",
    "testVariables = airTest.drop([<FILL IN>], axis = 1, inplace = False)\n",
    "\n",
    "print(trainVariables.head())\n",
    "print('------------------------------------------')\n",
    "print(testVariables.head())\n",
    "\n",
    "assert trainVariables.shape[1] == testVariables.shape[1] == 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modeling time!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# X is your DataFrame of predictors, y is a dataframe of labels\n",
    "linearModel = LinearRegression()\n",
    "linearModel.fit(X = <FILL IN>,\n",
    "                y = <FILL IN>)\n",
    "\n",
    "# Now join the true and predicted values together\n",
    "testPredictions = linearModel.predict(testVariables)\n",
    "testPredictionsDF = pd.DataFrame(testPredictions, columns=['prediction'])\n",
    "testComparison = pd.concat([testLabels.reset_index(drop=True), testPredictionsDF], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well we did! Since this was a regression problem, we'll use the Mean Squared Error (MSE) to evaluate the model.\n",
    "\n",
    "$$ \\begin{align}\n",
    " MSE & = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat Y_i)^2 \\\\\n",
    " & = \\frac{1}{n} \\sum_{i=1}^n (f(y_i) - \\hat f(y_i) )^2 \n",
    " \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.1715\n",
      "Root MSE = 0.4141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "# Let's see how well it did. The function below takes two columns, one of true values and \n",
    "# another with the predicted.\n",
    "\n",
    "error = mean_squared_error(testComparison[[0]], testComparison[[1]])\n",
    "print('MSE =', round(error, 4))\n",
    "print('Root MSE =', round(math.sqrt(error),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is in the units of the dependent variable, so now we can say... \"We expect that our model will predict the concentration of Carbon Monoxide within 0.41 $ \\frac{mg}{m^3} $ of the true value.\"\n",
    "\n",
    "If you finished early you can... work on DataCamp or Codecademy exercises, check out some of the cited concepts in this tutorial, or take a peek at next week's tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thanks to...\n",
    "\n",
    "- [The MNIST digits dataset from Kaggle](https://www.kaggle.com/c/digit-recognizer/data)\n",
    "- [The UCI Iris dataset](https://archive.ics.uci.edu/ml/datasets/Iris)\n",
    "- [The UCI Air Quality dataset](https://archive.ics.uci.edu/ml/datasets/Air+Quality)\n",
    "- [This **great** blog post on PCA](http://colah.github.io/posts/2014-10-Visualizing-MNIST/)\n",
    "- [This Git Blog](http://bdewilde.github.io/blog/blogger/2012/10/26/classification-of-hand-written-digits-3/) for the kNN illustration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
