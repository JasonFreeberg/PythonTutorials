{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science at UCSB\n",
    "\n",
    "# Python for Data Science: ML Crash Course\n",
    "\n",
    "## Jason Freeberg, Fall 2016 \n",
    "\n",
    "Okay! Today will be a crash course in machine learning. I'll explain things at a high level and use scikit learn to show real examples. Feature engineering is the creation or collection of predictors for a machine learning pipeline, so we're covering ML first.\n",
    "\n",
    "To make an oversimplification, let's assume we have some set of *p* predictors, **X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub> ... X<sub>p</sub> ** for *n* observations. Then we have a corresponding set of dependent *n* variables, **Y**. **X** could be a set of 100 people (n=100), each with 10 variables (p=10) like height, weight, sex, location, education level, and so on. And, in the same example, **Y** is that person's salary. What I just described is a *regression* problem. Where we have **X** and **Y**, and **Y** is a continuous variable. Now, from **X** and **Y** we can *learn* **F**, the mapping from **X** to **Y**... **F**(**X**) = **Y**. Or in matrix notation...\n",
    "\n",
    "$$ F \\left(\n",
    "\\begin{matrix}\n",
    "X_{1,1} & ... & X_{1,p} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "X_{n,1} & ... & X_{n,p} \\\\\n",
    "\\end{matrix}\n",
    "\\right) \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "Y_1 \\\\\n",
    "\\vdots \\\\\n",
    "Y_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There are two main branches of machine learning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "Like the example above, supervised learning involves using a set of *n* inputs, **X**, and *n* crorresponding outputs, **Y**, to build a statistical model that can then give predicted outputs from new, unseen inputs. As you might expect, this type of learning has broad applications to business, healthcare, and physics.\n",
    "\n",
    "\n",
    "#### Sci Kit Lean Example\n",
    "Similar to the problem above, imagine we have both **X** and **Y** and try to learn the mapping between them. But what if our dependant variable, **Y**, doesn't span the real numbers? There are many casses where we're trying to *classify* our outcomes... good or bad, alive or dead, pay or default... and these are **classification** problems. \n",
    "\n",
    "Moreover, we can have multiple classes in **Y**. Think of tax brackets, image recognition, or types of crime. Food for thought: we can turn a regression problem into a classification problem simply by *binning* our outcomes. Salary in dollars would become income brackets. Then we could use classification algorithms instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows = 150 \n",
      "N cols = 5\n",
      "Classes in the dependent var. = set(['Iris-virginica', 'Iris-setosa', 'Iris-versicolor'])\n",
      "   sepalLength  sepalWidth  petalLength  petalWidth        class\n",
      "0          5.1         3.5          1.4         0.2  Iris-setosa\n",
      "1          4.9         3.0          1.4         0.2  Iris-setosa\n",
      "2          4.7         3.2          1.3         0.2  Iris-setosa\n",
      "3          4.6         3.1          1.5         0.2  Iris-setosa\n",
      "4          5.0         3.6          1.4         0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "# Iris is a classic dataset. It holds various measurements of flowers and their species.\n",
    "# If we want to predict species from the measurements, what kind of problem are we \n",
    "# working with?\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import cross_validation, metrics\n",
    "import urllib2\n",
    "import os\n",
    "\n",
    "\n",
    "def read_csv_from_url(URL, columnNames):\n",
    "    response = urllib2.urlopen(URL)\n",
    "    lines = pd.read_csv(response,\n",
    "                        header=None,\n",
    "                        index_col=False)\n",
    "    dataframe = pd.DataFrame(lines)\n",
    "    dataframe.columns = columnNames\n",
    "    return dataframe\n",
    "\n",
    "# Seeds make our random methods reproducible.\n",
    "seed = 123\n",
    "\n",
    "# Load the Iris dataset\n",
    "irisURL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "irisNames = ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'class']\n",
    "iris = read_csv_from_url(irisURL, irisNames)\n",
    "print 'N rows =', iris.shape[0], '\\n', 'N cols =', iris.shape[1]\n",
    "print 'Classes in the dependent var. =', set(iris['class'])\n",
    "print iris.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Test Sets\n",
    "\n",
    "When you're studying for an exam at school you have your notes, review packets, and maybe some recorded lectures. You study, or *train*, with these materials and then you take the exam to see how well you understand the material you studied. The test has questions *similar* to what you studied, but not equal. So if you studied hard, you should be able to **generalize** well to these new \"inputs\".\n",
    "\n",
    "Likewise in machine learning, we need some data with the recorded inputs *and* outputs. Then when a model is trained on that data, we need to evaulate the model's perfomance by giving it new, unseen data. This is a common paradigm in machine and statistical learning, but it can be easy to mess up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: \n",
      "[[ 5.8  2.8  5.1  2.4]\n",
      " [ 6.3  3.4  5.6  2.4]\n",
      " [ 5.5  2.3  4.   1.3]\n",
      " [ 5.1  3.8  1.5  0.3]\n",
      " [ 4.4  3.   1.3  0.2]]\n",
      "Dependent variable: \n",
      "['Iris-virginica' 'Iris-virginica' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "train, test = cross_validation.train_test_split(iris,\n",
    "                                                test_size=0.3,\n",
    "                                               random_state=seed)\n",
    "\n",
    "# Coerce the independent and dependent variable of the training set to NumPy arrays.\n",
    "predictors = np.array(train.ix[:, 0:-1])\n",
    "variable = np.array(train.ix[:, -1])\n",
    "\n",
    "print 'Predictors:', '\\n', predictors[:5]\n",
    "print 'Dependent variable:', '\\n', variable[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Algorithm\n",
    "\n",
    "Since Iris is a classic dataset, we're going to use a classic classification algorithm. You'll probably see this example a lot in books and online, it's *classifying the Iris dataset with **k nearest neighbors***. In kNN, we put all of our predictors in a *feature space*, stored with their classifications. Then when a new observation needs to be classified, we compare it to the nearest **k** observations and give it the classification of the majority nearest **k** observations.\n",
    "\n",
    "In the diagram below, you can see that the predicted class for the star can change depending on the number of neighbors used.\n",
    "\n",
    "![image](http://bdewilde.github.io/assets/images/2012-10-26-knn-concept.png)\n",
    "\n",
    "*Food for thought*: kNN can also be used for regression problems! In a regression context, we predict based on the mean target of the k neighbors. For a more detailed explanation on kNN classification, [click here](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/). Now for the code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Confusion Matrix ------------------\n",
      "[[18  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  0 17]]\n",
      "-------------- Incorrect Classifications --------------\n",
      "             class      prediction\n",
      "0  Iris-versicolor  Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "# KNN object and fit\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X=predictors,\n",
    "        y=variable)\n",
    "\n",
    "# Make predictions on test set\n",
    "testPredictors = test.ix[:, :-1]\n",
    "actual = test.ix[:, -1]\n",
    "predictions = knn.predict(testPredictors)\n",
    "\n",
    "# Merge the predicted and actual classifications\n",
    "predictionsDF = pd.DataFrame(predictions, columns=['prediction'])\n",
    "results = pd.concat([test.reset_index(drop=True), predictionsDF], axis=1)\n",
    "\n",
    "# Model evaluation\n",
    "matrix = metrics.confusion_matrix(y_true=results['class'],\n",
    "                                  y_pred=results['prediction'])\n",
    "\n",
    "print '------------------ Confusion Matrix ------------------'\n",
    "print matrix\n",
    "\n",
    "incorrect = results.ix[results['class'] != results['prediction'], ['class', 'prediction']]\n",
    "if incorrect.shape[0] == 0:\n",
    "    print 'No incorrect classifications!'\n",
    "else:\n",
    "    print '-------------- Incorrect Classifications --------------'\n",
    "    print incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "Supervised learning is fairly straightforward, as we saw in the example. However, unsupervised learning requires some abstraction. Essentially, in an unsupervised exercise we are trying to either uncover hidden structure, find similarities, or reduce dimensionality in the data.\n",
    "\n",
    "A common unsupervised example is clustering. If I were to hand you a bucket of rocks and ask you to put them into groups you may look at features like weight, volume, color and texture. Then you can group them by those characteristics. \n",
    "\n",
    "In a machine learning context, we can use unsupervised clustering as a pre-processing step. After our observations are clustered, we can easily add their cluster numbers as a column of predictors in the training data. Now we can use that new variable as a predictor in a regression or classification problem!\n",
    "\n",
    "Eventually you may come across the problem of having *too many predictors*. If you have a set of 10 million observations and 200 predictors, then building a model on all predictors will be very expensive and time consuming. Thankfully, using techniques like [Principle Component Analysis](http://colah.github.io/posts/2014-10-Visualizing-MNIST/), we can reduce the number of predictors to only those with a *high variability* and save time in the long run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction using PCA\n",
    "\n",
    "We're going to exemplify unsupervised learning with another classic dataset: the MNIST collection of handwritten digits. Each digit is a 28x28 pixel image, for 784 total pixels... \n",
    "\n",
    "\\begin{matrix}\n",
    "000 & 001 & 002 & 003 & ... & 026 & 027 \\\\\n",
    "028 & 029 & 030 & 031 & ... & 054 & 055 \\\\\n",
    "056 & 057 & 058 & 059 & ... & 082 & 083 \\\\\n",
    " \\vdots &  \\vdots &  \\vdots &  \\vdots & ... & \\vdots & \\vdots \\\\\n",
    "728 & 729 & 730 & 731 & ... & 754 & 755 \\\\\n",
    "756 & 757 & 758 & 759 & ... & 782 & 783 \\\\\n",
    "\\end{matrix}\n",
    "\n",
    "The .csv contains the level of darkness for each pixel, organized as a table with 785 columns and 42,000 rows. This translates to the 784 pixel values *plus* the actual digit labels, and 42,000 digits...\n",
    "\n",
    "\\begin{matrix}\n",
    "'1' & 001 & 002 & 003 & 004 & ... & 783 & 784 \\\\\n",
    "'7' & 001 & 002 & 003 & 004 & ... & 783 & 784 \\\\\n",
    "'3' & 001 & 002 & 003 & 004 & ... & 783 & 784 \\\\\n",
    " \\vdots & \\vdots &  \\vdots &  \\vdots &  \\vdots & ... & \\vdots & \\vdots \\\\\n",
    "'8' & 001 & 002 & 003 & 004 & ... & 783 & 784 \\\\\n",
    "'6' & 001 & 002 & 003 & 004 & ... & 783 & 784 \\\\\n",
    "\\end{matrix}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows = 42000 \n",
      "N cols = 785\n",
      "------------------ Head of Data ------------------\n",
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read data from the repo you downloaded\n",
    "location = os.path.realpath(os.path.join(os.getcwd(), \"digits.csv\"))\n",
    "digits = pd.read_csv(location)\n",
    "\n",
    "# Let's peak at the data\n",
    "print \"N rows =\", digits.shape[0], '\\n', 'N cols =', digits.shape[1]\n",
    "print '------------------ Head of Data ------------------'\n",
    "print digits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, n_components=10, whiten=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "trainDigits, testDigits = cross_validation.train_test_split(digits,\n",
    "                                                            test_size=0.3,\n",
    "                                                            random_state=seed)\n",
    "# Coerce training data to np arrays\n",
    "trainLabel = trainDigits['label']\n",
    "trainPixels = trainDigits.ix[:, 1:]\n",
    "\n",
    "# Coerce testing data to np arrays\n",
    "testLabel = testDigits['label']\n",
    "testPixels = testDigits.ix[:, 1:]\n",
    "\n",
    "# Fit a PCA model\n",
    "pca = PCA(n_components = 10)\n",
    "pca.fit(X=trainPixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digits correctly classified = 12597\n",
      "Digits incorrectly classified = 3\n",
      "Raw accuracy = 0.9998\n"
     ]
    }
   ],
   "source": [
    "# Let's train a KMeans model with the 10 principle components.\n",
    "# First we need to transform the training set using the PCA parameters...\n",
    "trainPCAPixels = pca.transform(trainPixels)\n",
    "\n",
    "PCAdigitsKNN = KNeighborsClassifier()\n",
    "PCAdigitsKNN.fit(X=trainPCAPixels,\n",
    "              y=trainLabel)\n",
    "\n",
    "# And now make predictions on the test set.\n",
    "# First we need to transform the test set using the PCA parameters...\n",
    "testPCAPixels = pca.transform(testPixels)\n",
    "\n",
    "PCAdigitPredictions = PCAdigitsKNN.predict(testPCAPixels)\n",
    "PCAdigitPredictionsDF = pd.DataFrame(PCAdigitPredictions, columns=['prediction'])\n",
    "PCAdigitComparison = pd.concat([testLabel.reset_index(drop=True), PCAdigitPredictionsDF], axis=1)\n",
    "PCAdigitsIncorrect = results.ix[PCAdigitComparison['label'] != \\\n",
    "                             PCAdigitComparison['prediction'], ['class', 'prediction']].shape[0]\n",
    "\n",
    "PCAdigitsCorrect = PCAdigitComparison.shape[0] - PCAdigitsIncorrect\n",
    "print 'Digits correctly classified =', PCAdigitsCorrect \n",
    "print 'Digits incorrectly classified =', PCAdigitsIncorrect\n",
    "print 'Raw accuracy =', round(PCAdigitsCorrect / float(PCAdigitsCorrect + PCAdigitsIncorrect), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our accuracy of 99.98% to a baseline kNN using the full set of 784 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "digitsKNN = KNeighborsClassifier()\n",
    "digitsKNN.fit(X=trainPixels,\n",
    "             y=trainLabel)\n",
    "\n",
    "digitPredictions = digitsKNN.predict(testPixels)\n",
    "\n",
    "digitPredictionsDF = pd.DataFrame(digitPredictions, columns=['prediction'])\n",
    "digitComparison = pd.concat([testLabel.reset_index(drop=True), digitPredictionsDF], axis=1)\n",
    "digitsIncorrect = results.ix[digitComparison['label'] != \\\n",
    "                             digitComparison['prediction'], ['class', 'prediction']].shape[0]\n",
    "\n",
    "digitsCorrect = digitComparison.shape[0] - digitsIncorrect\n",
    "print 'Digits correctly classified =', digitsCorrect \n",
    "print 'Digits incorrectly classified =', digitsIncorrect\n",
    "print 'Raw accuracy =', round(digitsCorrect / float(digitsCorrect + digitsIncorrect), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thanks to...\n",
    "\n",
    "- [The MNIST digits dataset from Kaggle]()\n",
    "- [The UCI Iris dataset]()\n",
    "- [This **great** blog post on PCA](http://colah.github.io/posts/2014-10-Visualizing-MNIST/)\n",
    "- [This Git Blog](http://bdewilde.github.io/blog/blogger/2012/10/26/classification-of-hand-written-digits-3/) for the kNN illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'asd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
