{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science at UCSB\n",
    "\n",
    "# Python for Data Science: ML Crash Course\n",
    "\n",
    "## Jason Freeberg, Fall 2016 \n",
    "\n",
    "Okay! Today will be a crash course in machine learning. I'll explain things at a high level and use Sci Kit Learn to show real examples. Next week we will cover \"Feature engineering\", which is the creation or collection of predictors for a machine learning pipeline, so we need cover ML first.\n",
    "\n",
    "Alright let's make some gross oversimplifications! Assume we have some set of *p* predictors, **X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub> ... X<sub>p</sub> ** for *n* observations. Assume we also have a corresponding set of *n* dependent variables, **Y**. Our matrix, **X**, could be a set of 100 people (n=100), each with 10 variables (p=10) like height, weight, sex, location, education level, and so on. And in the same example, **Y** could be that person's salary. What I just described is a *regression* problem. When we know **X** and **Y**, where **Y** is a continuous variable. Now, from **X** and **Y** we can *learn* **F**, the mapping from **X** to **Y**. We can express it as **F**(**X**) = **Y** or in matrix notation as...\n",
    "\n",
    "$$ F \\left(\n",
    "\\begin{matrix}\n",
    "X_{1,1} & ... & X_{1,p} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "X_{n,1} & ... & X_{n,p} \\\\\n",
    "\\end{matrix}\n",
    "\\right) \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "Y_1 \\\\\n",
    "\\vdots \\\\\n",
    "Y_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There are two main branches of machine learning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "Like the example above, supervised learning involves using a matrix of *n* by *p* inputs, **X**, and *n* crorresponding outputs, **Y**, to build a statistical model that can then give predictions from new, unseen inputs. As you might expect, this type of learning has broad applications in business, healthcare, and physics.\n",
    "\n",
    "In these problems, we assume that there is a true, unknown function of the form below, where espison is an irriducible error term with a variance centered around zero. We are attempting to approximate **F** as closely as possible because we cannot reduce epsilon--it cannot be mitigated. Let $ E(Y - \\hat Y)^2 $ be the average squared difference between the model's prediction and the *true* value.\n",
    "\n",
    "$$ Y = f(X) + \\epsilon $$\n",
    "\n",
    "$$ ... $$\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    " E(Y - \\hat Y)^2 & = E(f(X) + \\epsilon - \\hat f(X))^2 \\\\\n",
    "& = (f(X) - \\hat f(X))^2  + Var(\\epsilon) \\\\\n",
    " \\end{align}$$\n",
    "\n",
    "We can see that minimizing the difference between the actual and the predicted output is where we can improve the model.\n",
    "\n",
    "#### Sci Kit Lean Example\n",
    "Similar to the problem above, imagine we have both **X** and **Y** and try to learn the mapping between them. But what if our dependant variable, **Y**, doesn't span the real numbers? There are many casses where we're trying to *classify* our outcomes... the dependent variable could be good or bad, alive or dead, paid or defaulted... and these are **quantitative** or **classification** problems. \n",
    "\n",
    "Moreover, we can have multiple classes in **Y**, not just binary output! Think of tax brackets, image recognition, or types of crime. Food for thought: we can turn a regression problem into a classification problem simply by *binning* our outcomes. A person's salary in dollars would become income brackets. Then we could use classification algorithms instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows = 150\n",
      "Number of columns = 5\n",
      "Classes in the dependent variable = ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica'] \n",
      "\n",
      "   sepalLength  sepalWidth  petalLength  petalWidth        class\n",
      "0          5.1         3.5          1.4         0.2  Iris-setosa\n",
      "1          4.9         3.0          1.4         0.2  Iris-setosa\n",
      "2          4.7         3.2          1.3         0.2  Iris-setosa\n",
      "3          4.6         3.1          1.5         0.2  Iris-setosa\n",
      "4          5.0         3.6          1.4         0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "# Iris is a classic dataset. It holds various measurements of flowers and their species.\n",
    "# If we want to predict species from the measurements, what kind of problem are we \n",
    "# working with?\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import cross_validation, metrics\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "\n",
    "\n",
    "def read_csv_from_url(URL, columnNames):\n",
    "    response = urlopen(URL)\n",
    "    lines = pd.read_csv(response,\n",
    "                        header=None,\n",
    "                        index_col=False)\n",
    "    dataframe = pd.DataFrame(lines)\n",
    "    dataframe.columns = columnNames\n",
    "    return dataframe\n",
    "\n",
    "# Seeds make our random methods reproducible.\n",
    "seed = 123\n",
    "\n",
    "# Load the Iris dataset using the UCI URL, the column names, and the function\n",
    "# defined above.\n",
    "irisURL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "irisNames = ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'class']\n",
    "iris = read_csv_from_url(irisURL, irisNames)\n",
    "\n",
    "# Print some info on our data. Number of rows, columns, classes in Y, and the head.\n",
    "print('Number of rows =', iris.shape[0]) \n",
    "print('Number of columns =', iris.shape[1])\n",
    "print('Classes in the dependent variable =', (iris['class'].unique()), '\\n')\n",
    "print(iris.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Test Sets\n",
    "\n",
    "This is important... When you are studying for an exam at school you have your notes, review packets, and maybe some recorded lectures. You study, or *train*, with these materials and then you take the exam to see how well you understand the material you studied. The test has questions *similar* to what you studied, but not exactly the same. So if you studied hard, your mind should be able to **generalize** wat you learned to these new \"inputs\".\n",
    "\n",
    "Likewise in machine learning, we need some data with the recorded inputs **and** outputs. Then a model is trained on that data, and we can evaulate the model's perfomance by giving it new, unseen data. This is a common paradigm in machine and statistical learning, but it can be easy to mess up. More robust methods of model evaluation include [k-fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics) and [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: \n",
      " [[ 5.8  2.8  5.1  2.4]\n",
      " [ 6.3  3.4  5.6  2.4]\n",
      " [ 5.5  2.3  4.   1.3]\n",
      " [ 5.1  3.8  1.5  0.3]\n",
      " [ 4.4  3.   1.3  0.2]]\n",
      "Dependent variable: \n",
      " ['Iris-virginica' 'Iris-virginica' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "# Notice the \"random_seed\" argument\n",
    "train, test = cross_validation.train_test_split(iris,\n",
    "                                                test_size=0.3,\n",
    "                                                random_state=seed)\n",
    "\n",
    "# Coerce the independent and dependent variable of the training set to NumPy arrays.\n",
    "predictors = np.array(train.ix[:, 0:-1])\n",
    "variable = np.array(train.ix[:, -1])\n",
    "\n",
    "print('Predictors:', '\\n', predictors[:5])\n",
    "print('Dependent variable:', '\\n', variable[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Algorithm\n",
    "\n",
    "Since Iris is a classic dataset, we're going to use a classic classification algorithm. You'll probably see this example often in books and online, it's *classifying the Iris dataset with **k nearest neighbors***. In kNN, we put all of our predictors in a *feature space*, stored with their classifications. Then when a new observation needs to be classified, we compare it to the nearest **k** observations and give it the classification of the majority nearest **k** observations.\n",
    "\n",
    "In the diagram below, you can see that the predicted class for the star can change depending on the number of neighbors used.\n",
    "\n",
    "![image](http://bdewilde.github.io/assets/images/2012-10-26-knn-concept.png)\n",
    "\n",
    "*Food for thought*: kNN can also be used for regression problems! In a regression context, we predict based on the mean target of the k neighbors. For a more detailed explanation on kNN classification, [click here](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/). Now for the code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Confusion Matrix ------------------\n",
      "[[18  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  0 17]]\n",
      "-------------- Incorrect Classifications --------------\n",
      "             class      prediction\n",
      "0  Iris-versicolor  Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a KNN object and fit the data\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X=predictors,\n",
    "        y=variable)\n",
    "\n",
    "# Make predictions on test set\n",
    "testPredictors = test.ix[:, :-1]\n",
    "actual = test.ix[:, -1]\n",
    "predictions = knn.predict(testPredictors)\n",
    "\n",
    "# Merge the predicted and actual classifications\n",
    "predictionsDF = pd.DataFrame(predictions, columns=['prediction'])\n",
    "results = pd.concat([test.reset_index(drop=True), predictionsDF], axis=1)\n",
    "\n",
    "# Model evaluation\n",
    "matrix = metrics.confusion_matrix(y_true=results['class'],\n",
    "                                  y_pred=results['prediction'])\n",
    "\n",
    "print('------------------ Confusion Matrix ------------------')\n",
    "print(matrix)\n",
    "\n",
    "incorrect = results.ix[results['class'] != results['prediction'], ['class', 'prediction']]\n",
    "\n",
    "if incorrect.shape[0] == 0:\n",
    "    print('No incorrect classifications!')\n",
    "else:\n",
    "    print('-------------- Incorrect Classifications --------------')\n",
    "    print(incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "Supervised learning is fairly straightforward, as we saw in the example. However, unsupervised learning requires some abstraction. Essentially, in an unsupervised learning exercise we are trying to either uncover hidden structure, find similarities, or reduce dimensionality in the data.\n",
    "\n",
    "A common unsupervised example is clustering. If I were to hand you a bucket of rocks and ask you to put them into groups you may look at features like weight, volume, color and texture. Then you can group them by those characteristics. \n",
    "\n",
    "In a machine learning context, we can use unsupervised clustering as a pre-processing step. After our observations are clustered, we can easily add their cluster identifications as a column of predictors in the training data. Now we can use that new variable as a predictor in a regression or classification problem! **Neat!**\n",
    "\n",
    "Eventually you may come across the problem of having *too many predictors*. If you have a set of 10 million observations and 2000 predictors, then building a model on all predictors will be very expensive and time consuming. Thankfully, using techniques like [Principle Component Analysis](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) we can reduce the number of predictors to only those with a *high variability* and save time in the long run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction using PCA\n",
    "\n",
    "We're going to work through an unsupervised learning example with another classic dataset: the MNIST collection of handwritten digits. Each digit is an image containing 28x28 pixels, for 784 total pixels laid out as below. Each pixel contains a number in the range from 0 to 1. 0 codes for a white pixel, 1 codes for a black pixel.\n",
    "\n",
    "\\begin{matrix}\n",
    "000 & 001 & 002 & 003 & ... & 026 & 027 \\\\\n",
    "028 & 029 & 030 & 031 & ... & 054 & 055 \\\\\n",
    "056 & 057 & 058 & 059 & ... & 082 & 083 \\\\\n",
    " \\vdots &  \\vdots &  \\vdots &  \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "728 & 729 & 730 & 731 & ... & 754 & 755 \\\\\n",
    "756 & 757 & 758 & 759 & ... & 782 & 783 \\\\\n",
    "\\end{matrix}\n",
    "\n",
    "The .csv contains this pixel data for 42,000 digits, organized as a table with 785 columns and 42,000 rows. This translates to the same 784 pixel values *plus* the actual digit labels. And with 42,000 digits we get a DataFrame laid out as so...\n",
    "\n",
    "\\begin{matrix}\n",
    "\"2\" & 000 & 001 & 002 & 003 & ... & 782 & 783 \\\\\n",
    "\"4\" & 000 & 001 & 002 & 003 & ... & 782 & 783 \\\\\n",
    "\"7\" & 000 & 001 & 002 & 003 & ... & 782 & 783 \\\\\n",
    "\"3\" & 000 & 001 & 002 & 003 & ... & 782 & 783 \\\\\n",
    " \\vdots & \\vdots &  \\vdots &  \\vdots &  \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "\"8\" & 000 & 001 & 002 & 003 & ... & 782 & 783 \\\\\n",
    "\"6\" & 000 & 001 & 002 & 003 & ... & 782 & 783 \\\\\n",
    "\"4\" & 000 & 001 & 002 & 003 & ... & 782 & 783 \\\\\n",
    "\\end{matrix}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows = 42000 \n",
      " N cols = 785\n",
      "------------------ Head of Data ------------------\n",
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read data from the repo you downloaded or cloned\n",
    "location = os.path.realpath(os.path.join(os.getcwd(), \"digits.csv\"))\n",
    "digits = pd.read_csv(location)\n",
    "\n",
    "# Let's peak at the data\n",
    "print(\"N rows =\", digits.shape[0], '\\n', 'N cols =', digits.shape[1])\n",
    "print('------------------ Head of Data ------------------')\n",
    "print(digits.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, n_components=10, whiten=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "trainDigits, testDigits = cross_validation.train_test_split(digits,\n",
    "                                                            test_size=0.3,\n",
    "                                                            random_state=seed)\n",
    "# Coerce training data to np arrays.\n",
    "# One array for the true labels, another \n",
    "# for the predictors (pixels).\n",
    "trainLabel = trainDigits['label']\n",
    "trainPixels = trainDigits.ix[:, 1:]\n",
    "\n",
    "# Coerce testing data to np arrays.\n",
    "# One array for the true labels, another\n",
    "# for the predictors (pixels).\n",
    "testLabel = testDigits['label']\n",
    "testPixels = testDigits.ix[:, 1:]\n",
    "\n",
    "# Fit a PCA object on the training set.\n",
    "pca = PCA(n_components = 10)\n",
    "pca.fit(X=trainPixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digits correctly classified = 12597\n",
      "Digits incorrectly classified = 3\n",
      "Raw accuracy = 0.9998\n"
     ]
    }
   ],
   "source": [
    "# Let's train a kNN model with the 10 principle components.\n",
    "# First we need to transform the training set using the PCA loadings...\n",
    "trainPCAPixels = pca.transform(trainPixels)\n",
    "\n",
    "PCAdigitsKNN = KNeighborsClassifier()\n",
    "PCAdigitsKNN.fit(X=trainPCAPixels,\n",
    "                 y=trainLabel)\n",
    "\n",
    "# Now make predictions on the test set.\n",
    "# We need to also transform the test set using the PCA loadings...\n",
    "testPCAPixels = pca.transform(testPixels)\n",
    "\n",
    "PCAdigitPredictions = PCAdigitsKNN.predict(testPCAPixels)\n",
    "PCAdigitPredictionsDF = pd.DataFrame(PCAdigitPredictions, columns=['prediction'])\n",
    "PCAdigitComparison = pd.concat([testLabel.reset_index(drop=True), PCAdigitPredictionsDF], axis=1)\n",
    "PCAdigitsIncorrect = results.ix[PCAdigitComparison['label'] != \\\n",
    "                             PCAdigitComparison['prediction'], ['class', 'prediction']].shape[0]\n",
    "\n",
    "PCAdigitsCorrect = PCAdigitComparison.shape[0] - PCAdigitsIncorrect\n",
    "print('Digits correctly classified =', PCAdigitsCorrect )\n",
    "print('Digits incorrectly classified =', PCAdigitsIncorrect)\n",
    "print('Raw accuracy =', round(PCAdigitsCorrect / float(PCAdigitsCorrect + PCAdigitsIncorrect), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our accuracy\\* of 99.98% to a baseline kNN using the full set of 784 features. Since we have over 70 times the predictors, the computation takes quite a while. For that reason, I downsampled the training data. If you want to see a *true* comparison, then comment out the sampling and **get ready to wait.**\n",
    "\n",
    "\\* *Accuracy is a poor metric to evaluate classifiers. Example: We are classifying patients as being sick or healthy, and 99% of our population is healthy. We could achieve 99% accuracy by simply predicting that every person will be healthy! Check out [sensitivity and specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) and [area under the ROC curve](http://gim.unmc.edu/dxtests/roc3.htm) as better metrics for model evaluation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell takes a while to run. If you want to see the results, delete the block comment lines (\"\"\").\n",
    "\n",
    "\"\"\"\n",
    "digits2 = trainDigits.sample(frac=0.5,\n",
    "                                random_state=seed)\n",
    "\n",
    "trainDigits2, testDigits2= cross_validation.train_test_split(digits2,\n",
    "                                                            test_size=0.3,\n",
    "                                                            random_state=seed)\n",
    "\n",
    "# Get the pixels and labels from train data\n",
    "trainLabels2 = trainDigits2['label']\n",
    "trainPixels2 = trainDigits2.ix[:, 1:]\n",
    "\n",
    "# And the same from the testing data\n",
    "testLabels2 = testDigits2['label']\n",
    "testPixels2 = testDigits2.ix[:, 1:]\n",
    "\n",
    "digitsKNN = KNeighborsClassifier()\n",
    "digitsKNN.fit(X=trainPixels2,\n",
    "             y=trainLabels2)\n",
    "\n",
    "digitPredictions = digitsKNN.predict(testPixels2)\n",
    "\n",
    "digitPredictionsDF = pd.DataFrame(digitPredictions, columns=['prediction'])\n",
    "digitComparison = pd.concat([testLabels2.reset_index(drop=True), digitPredictionsDF], axis=1)\n",
    "digitsIncorrect = results.ix[digitComparison['label'] != \\\n",
    "                             digitComparison['prediction'], ['class', 'prediction']].shape[0]\n",
    "\n",
    "digitsCorrect = digitComparison.shape[0] - digitsIncorrect\n",
    "print 'Digits correctly classified =', digitsCorrect \n",
    "print 'Digits incorrectly classified =', digitsIncorrect\n",
    "print 'Raw accuracy =', round(digitsCorrect / float(digitsCorrect + digitsIncorrect), 4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Your turn!\n",
    "\n",
    "Alrighty! With the class time left you're going to load **and clean** the UCI dataset containing hourly measurements of road-level air quality in a busy Italian city. Then you will build a multiple [linear regression](https://en.wikipedia.org/wiki/Linear_regression) model to predict the concentration of Carbon Monoxide, 'CO(GT)', from the predictors. \n",
    "\n",
    "Consult the [SciKit Learn docs](http://scikit-learn.org/stable/modules/classes.html) and the earlier code to get help. You can also ask Jason, but give it a solid attempt before asking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date      Time  CO(GT)  PT08.S1(CO)  NMHC(GT)  C6H6(GT)  \\\n",
      "0  10/03/2004  18.00.00     2.6       1360.0     150.0      11.9   \n",
      "1  10/03/2004  19.00.00     2.0       1292.0     112.0       9.4   \n",
      "2  10/03/2004  20.00.00     2.2       1402.0      88.0       9.0   \n",
      "3  10/03/2004  21.00.00     2.2       1376.0      80.0       9.2   \n",
      "4  10/03/2004  22.00.00     1.6       1272.0      51.0       6.5   \n",
      "5  10/03/2004  23.00.00     1.2       1197.0      38.0       4.7   \n",
      "6  11/03/2004  00.00.00     1.2       1185.0      31.0       3.6   \n",
      "7  11/03/2004  01.00.00     1.0       1136.0      31.0       3.3   \n",
      "8  11/03/2004  02.00.00     0.9       1094.0      24.0       2.3   \n",
      "9  11/03/2004  03.00.00     0.6       1010.0      19.0       1.7   \n",
      "\n",
      "   PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)  \\\n",
      "0         1046.0    166.0        1056.0    113.0        1692.0       1268.0   \n",
      "1          955.0    103.0        1174.0     92.0        1559.0        972.0   \n",
      "2          939.0    131.0        1140.0    114.0        1555.0       1074.0   \n",
      "3          948.0    172.0        1092.0    122.0        1584.0       1203.0   \n",
      "4          836.0    131.0        1205.0    116.0        1490.0       1110.0   \n",
      "5          750.0     89.0        1337.0     96.0        1393.0        949.0   \n",
      "6          690.0     62.0        1462.0     77.0        1333.0        733.0   \n",
      "7          672.0     62.0        1453.0     76.0        1333.0        730.0   \n",
      "8          609.0     45.0        1579.0     60.0        1276.0        620.0   \n",
      "9          561.0   -200.0        1705.0   -200.0        1235.0        501.0   \n",
      "\n",
      "      T    RH      AH  Unnamed: 15  Unnamed: 16  \n",
      "0  13.6  48.9  0.7578          NaN          NaN  \n",
      "1  13.3  47.7  0.7255          NaN          NaN  \n",
      "2  11.9  54.0  0.7502          NaN          NaN  \n",
      "3  11.0  60.0  0.7867          NaN          NaN  \n",
      "4  11.2  59.6  0.7888          NaN          NaN  \n",
      "5  11.2  59.2  0.7848          NaN          NaN  \n",
      "6  11.3  56.8  0.7603          NaN          NaN  \n",
      "7  10.7  60.0  0.7702          NaN          NaN  \n",
      "8  10.7  59.7  0.7648          NaN          NaN  \n",
      "9  10.3  60.2  0.7517          NaN          NaN  \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ec4a2e8a4f09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mairQuality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mairQuality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run this cell to load the dataset and print the head.\n",
    "# THEN COME BACK, uncomment and complete the line to remove the empty columns\n",
    "\n",
    "location2 = os.path.realpath(os.path.join(os.getcwd(), \"AirQualityUCI.csv\"))\n",
    "airQuality = pd.read_csv(location2, delimiter = ';', decimal = ',')\n",
    "\n",
    "# FINISH THIS LINE AFTER RUNNING:\n",
    "#airQuality.drop(airQuality[<FILL IN>], axis=1, inplace=True)\n",
    "\n",
    "print(airQuality.head(10))\n",
    "\n",
    "# This assertion will catch until you drop the empty columns.\n",
    "assert airQuality.shape[1] == 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'airQuality' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b71b469ed730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Hint for the first <FILL IN>: the values can happen in any row or column.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mairQuality\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mairQuality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'N rows ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mairQuality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'N cols ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mairQuality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'airQuality' is not defined"
     ]
    }
   ],
   "source": [
    "# The air sensor sometimes failed, and mising values were replaced with -200. So let's replace\n",
    "# that with NumPy's NaN value.\n",
    "\n",
    "# Hint for the first <FILL IN>... the wrong values can happen in any row or column.\n",
    "airQuality[airQuality.ix[<FILL IN>, <FILL IN>] == -200] = np.nan\n",
    "\n",
    "print('N rows =', airQuality.shape[0], '\\n', 'N cols =', airQuality.shape[1])\n",
    "print(airQuality.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6941, 14)\n",
      "         Date      Time  CO(GT)  PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)  \\\n",
      "0  10/03/2004  18.00.00     2.6       1360.0      11.9         1046.0   \n",
      "1  10/03/2004  19.00.00     2.0       1292.0       9.4          955.0   \n",
      "2  10/03/2004  20.00.00     2.2       1402.0       9.0          939.0   \n",
      "3  10/03/2004  21.00.00     2.2       1376.0       9.2          948.0   \n",
      "4  10/03/2004  22.00.00     1.6       1272.0       6.5          836.0   \n",
      "\n",
      "   NOx(GT)  PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)     T    RH  \\\n",
      "0    166.0        1056.0    113.0        1692.0       1268.0  13.6  48.9   \n",
      "1    103.0        1174.0     92.0        1559.0        972.0  13.3  47.7   \n",
      "2    131.0        1140.0    114.0        1555.0       1074.0  11.9  54.0   \n",
      "3    172.0        1092.0    122.0        1584.0       1203.0  11.0  60.0   \n",
      "4    131.0        1205.0    116.0        1490.0       1110.0  11.2  59.6   \n",
      "\n",
      "       AH  \n",
      "0  0.7578  \n",
      "1  0.7255  \n",
      "2  0.7502  \n",
      "3  0.7867  \n",
      "4  0.7888  \n",
      "Date             0\n",
      "Time             0\n",
      "CO(GT)           0\n",
      "PT08.S1(CO)      0\n",
      "C6H6(GT)         0\n",
      "PT08.S2(NMHC)    0\n",
      "NOx(GT)          0\n",
      "PT08.S3(NOx)     0\n",
      "NO2(GT)          0\n",
      "PT08.S4(NO2)     0\n",
      "PT08.S5(O3)      0\n",
      "T                0\n",
      "RH               0\n",
      "AH               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Holy shit! That hyrdocarbon sensor really sucks! \n",
    "# For the sake of time, let's just drop that column too and remove rows with any NaNs.\n",
    "\n",
    "airQuality.drop(<FILL IN>, axis=1, inplace=True)\n",
    "airQuality.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "print(airQuality.shape)\n",
    "print(airQuality.head())\n",
    "print(airQuality.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  NO2(GT)  \\\n",
      "6968       1053.0       3.5          680.0    166.0         890.0    108.0   \n",
      "229        1064.0       4.3          728.0    115.0        1161.0     91.0   \n",
      "6645       1250.0      14.5         1133.0    508.0         629.0    168.0   \n",
      "9199        963.0       2.9          646.0    117.0         834.0     83.0   \n",
      "221        1215.0       8.3          912.0    127.0         948.0    109.0   \n",
      "\n",
      "      PT08.S4(NO2)  PT08.S5(O3)     T    RH      AH  \n",
      "6968        1166.0        807.0  11.3  80.1  1.0741  \n",
      "229         1461.0        842.0  13.5  58.4  0.8960  \n",
      "6645        1274.0       1463.0  17.5  30.8  0.6095  \n",
      "9199        1181.0        846.0  14.7  66.4  1.1006  \n",
      "221         1547.0        993.0  14.2  58.3  0.9380  \n",
      "------------------------------------------\n",
      "      PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  NO2(GT)  \\\n",
      "7144       1016.0       4.9          760.0    254.0         835.0    112.0   \n",
      "7236        939.0       1.8          568.0    101.0        1037.0     72.0   \n",
      "4668       1013.0       8.1          905.0    231.0         804.0     79.0   \n",
      "8401       1194.0      11.7         1038.0    388.0         605.0    223.0   \n",
      "1979        854.0       2.4          613.0     39.0        1272.0     41.0   \n",
      "\n",
      "      PT08.S4(NO2)  PT08.S5(O3)     T    RH      AH  \n",
      "7144        1044.0       1016.0   4.1  65.8  0.5451  \n",
      "7236        1054.0        656.0  12.4  62.6  0.8994  \n",
      "4668        1499.0        974.0  18.3  65.5  1.3686  \n",
      "8401        1317.0       1181.0   4.8  78.6  0.6826  \n",
      "1979        1515.0        565.0  16.8  81.3  1.5452  \n"
     ]
    }
   ],
   "source": [
    "# Great, now our data is free of missing values and we still have 7,400 observations!\n",
    "# Now let's fit a model!\n",
    "\n",
    "# Split the train and test sets into TWO dataframes: airTrain, and airTest\n",
    "<FILL IN>, <FILL IN> = cross_validation.train_test_split(airQuality, \n",
    "                                                      test_size=0.3,\n",
    "                                                      random_state=seed)\n",
    "Ncolumns = airTrain.shape[1]\n",
    "\n",
    "# Pull out the labels from the train and test sets. (label == dependent variable)\n",
    "trainLabels = airTrain[<FILL IN>]\n",
    "testLabels = airTest[<FILL IN>]\n",
    "\n",
    "# Pull out the predictors from the train and test sets\n",
    "# Do NOT select 'Date' or 'Time' as pedictors\n",
    "trainVariables = airTrain.drop([<FILL IN>], axis = 1, inplace = False)\n",
    "testVariables = airTest.drop([<FILL IN>], axis = 1, inplace = False)\n",
    "\n",
    "print(trainVariables.head())\n",
    "print('------------------------------------------')\n",
    "print(testVariables.head())\n",
    "\n",
    "assert trainVariables.shape[1] == testVariables.shape[1] == 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modeling time!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# X is your DataFrame of predictors, y is a dataframe of labels\n",
    "linearModel = LinearRegression()\n",
    "linearModel.fit(X = <FILL IN>,\n",
    "                y = <FILL IN>)\n",
    "\n",
    "# Now join the true and predicted values together\n",
    "testPredictions = linearModel.predict(testVariables)\n",
    "testPredictionsDF = pd.DataFrame(testPredictions, columns=['prediction'])\n",
    "testComparison = pd.concat([testLabels.reset_index(drop=True), testPredictionsDF], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well we did! Since this was a regression problem, weuse the Mean Squared Error (MSE) to evaluate the model.\n",
    "\n",
    "$$ \\begin{align}\n",
    " MSE & = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat Y_i)^2 \\\\\n",
    " & = \\frac{1}{n} \\sum_{i=1}^n (f(y_i) - \\hat f(y_i) )^2 \n",
    " \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.1715\n",
      "Root MSE = 0.4141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# Let's see how well it did. The function below takes two columns, one of true values and \n",
    "# another with the predicted.\n",
    "\n",
    "error = mean_squared_error(testComparison[[0]], testComparison[[1]])\n",
    "print('MSE =', round(error, 4))\n",
    "print('Root MSE =', round(math.sqrt(error),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is in the units of the dependent variable, so now we can say... \"We expect that our model will  generally predict the concentration of Carbon Monoxide within 0.41 $ \\frac{mg}{m^3} $ of the true value.\" If you're a statistics major and cringed because I didn't include a confidence interval, good. Remember this is a tutorial to get everyone's feet wet.\n",
    "\n",
    "If you finished early you can work on DataCamp or Codecademy exercises, check out some of the cited concepts in this tutorial, or take a peek at next week's tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thanks to...\n",
    "\n",
    "- [The MNIST digits dataset from Kaggle](https://www.kaggle.com/c/digit-recognizer/data)\n",
    "- [The UCI Iris dataset](https://archive.ics.uci.edu/ml/datasets/Iris)\n",
    "- [The UCI Air Quality dataset](https://archive.ics.uci.edu/ml/datasets/Air+Quality)\n",
    "- [This **great** blog post on PCA](http://colah.github.io/posts/2014-10-Visualizing-MNIST/)\n",
    "- [This Git Blog](http://bdewilde.github.io/blog/blogger/2012/10/26/classification-of-hand-written-digits-3/) for the kNN illustration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
