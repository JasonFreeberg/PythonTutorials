{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science at UCSB\n",
    "\n",
    "# Python for Data Science: Feature Engineering\n",
    "\n",
    "## Jason Freeberg, Fall 2016\n",
    "\n",
    "Ahoy! Now that we have a basic understanding of machine learning, today we'll go over feature engineering, or, *the process of adding predictors to strengthen the model of a machine learning pipeline*. Let's say a company is building a model to predict trends in the stock market, they will likely begin with historical data on the NASDAQ, Dow Jones, and major stock market leaders like Alphabet Inc., Ford, large oil companies, and banks.\n",
    "\n",
    "Then, as the team attempts to strengthen the predictability of the model, they could choose to include the Consumer Price Index (CPA), national trends in weather, or even a rolling [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) of the Wall Street Journal... for example. These are now new columns, predictors, or *features*, in our datatable to help our model predict the stock market.\n",
    "\n",
    "Depending on who you ask, [Interaction terms](https://en.wikipedia.org/wiki/Interaction_(statistics)), [scaling](scaling predictors), [centering](http://www.theanalysisfactor.com/center-on-the-mean/), and [standarizing](https://en.wikipedia.org/wiki/Standard_score) could also fall under feature engineering. \n",
    "\n",
    "Since the general idea here is simple, we will spend most of today working through the nitty gritty of adding predictors to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data\n",
    "\n",
    "I wanted to use a time-series dataset to continue the example from earlier, but the the support for time series modeling in Python is limited. Instead we you guts will play with a dataset fit for classification: the historical San Francisco crime breakdown from 2003 to 2015...courtest of [Kaggle](https://www.kaggle.com/c/sf-crime). Then we'll try adding and mutating our predictors. In a production model, a feature engineer might spend weeks building a web scraper, gathering gigabytes of relevant information, and parsing it down to well formated features. \n",
    "\n",
    "BUT! In today's lab we will simply leave out some given predictors, then add them in to assess their significance ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nrow = 5043. Ncol = 28\n",
      "-------------------- Column Names and Examples --------------------\n",
      "color                           Color               \n",
      "director_name                   Sam Raimi           \n",
      "num_critic_for_reviews          392.0               \n",
      "duration                        156.0               \n",
      "director_facebook_likes         0.0                 \n",
      "actor_3_facebook_likes          4000.0              \n",
      "actor_2_name                    James Franco        \n",
      "actor_1_facebook_likes          24000.0             \n",
      "gross                           336530303.0         \n",
      "genres                          Action|Adventure|Romance\n",
      "actor_1_name                    J.K. Simmons        \n",
      "movie_title                     Spider-Man 3Â        \n",
      "num_voted_users                 383056              \n",
      "cast_total_facebook_likes       46055               \n",
      "actor_3_name                    Kirsten Dunst       \n",
      "facenumber_in_poster            0.0                 \n",
      "plot_keywords                   sandman|spider man|symbiote|venom|villain\n",
      "movie_imdb_link                 http://www.imdb.com/title/tt0413300/?ref_=fn_tt_tt_1\n",
      "num_user_for_reviews            1902.0              \n",
      "language                        English             \n",
      "country                         USA                 \n",
      "content_rating                  PG-13               \n",
      "budget                          258000000.0         \n",
      "title_year                      2007.0              \n",
      "actor_2_facebook_likes          11000.0             \n",
      "imdb_score                      6.2                 \n",
      "aspect_ratio                    2.35                \n",
      "movie_facebook_likes            0                   \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import cross_validation, metrics\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "\n",
    "seed = 123\n",
    "\n",
    "# I'll let you guys play with the San Francisco dataset, I'll use this set from IMDB. \n",
    "# Let's try to predict the score from some of the other predictors.\n",
    "\n",
    "location = os.path.realpath(os.path.join(os.getcwd(), \"movie_metadata.csv\"))\n",
    "movieData = pd.read_csv(location)\n",
    "\n",
    "print('Nrow = {0}. Ncol = {1}'.format(movieData.shape[0], movieData.shape[1]))\n",
    "print('-------------------- Column Names and Examples --------------------')\n",
    "for col in range(movieData.shape[1]):\n",
    "    print('{0:30}  {1:20}'.format(str(movieData.columns[col]), str(movieData.ix[6, col])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb_score                   float64\n",
      "color                         object\n",
      "title_year                   float64\n",
      "cast_total_facebook_likes      int64\n",
      "budget                       float64\n",
      "dtype: object\n",
      "------- Missing Data: -------\n",
      "imdb_score                     0\n",
      "color                         19\n",
      "title_year                   108\n",
      "cast_total_facebook_likes      0\n",
      "budget                       492\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "startingPreds = movieData[['imdb_score', 'color', 'title_year', 'cast_total_facebook_likes', 'budget']].copy()\n",
    "\n",
    "# Let's print our starting predictors\n",
    "\n",
    "print(startingPreds.dtypes)\n",
    "print('------- Missing Data: -------')\n",
    "print(startingPreds.isnull().sum())\n",
    "\n",
    "# We could spend a whole tutorial on ways to deal with missing data. But for the sake of time \n",
    "# we'll just drop rows with missing predictors.\n",
    "\n",
    "startingPreds.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# Encode our categorical variable\n",
    "\n",
    "colorCode = LabelEncoder().fit(startingPreds['color'])\n",
    "startingPreds['color'] = colorCode.transform(startingPreds['color'])\n",
    "\n",
    "# Model building\n",
    "\n",
    "movieTrain, movieTest = cross_validation.train_test_split(startingPreds, \n",
    "                                                          test_size = 0.3, \n",
    "                                                          random_state = seed)\n",
    "\n",
    "linearModel = LinearRegression().fit(X = movieTrain.ix[:, movieTrain.columns != 'imdb_score'], \n",
    "                                     y = movieTrain['imdb_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our initial root MSE = 1.112\n"
     ]
    }
   ],
   "source": [
    "# Test and evaluate model\n",
    "\n",
    "moviePredictions = pd.DataFrame(linearModel.predict(movieTest.ix[:, movieTest.columns != 'imdb_score']),\n",
    "                                columns = ['predictions'])\n",
    "firstResults = pd.concat([moviePredictions, movieTest['imdb_score'].reset_index(drop=True)], axis = 1)\n",
    "\n",
    "firstMSE = metrics.mean_squared_error(y_true = firstResults['imdb_score'],\n",
    "                                         y_pred = firstResults['predictions'])\n",
    "\n",
    "firstRootMSE = np.sqrt(firstMSE)\n",
    "\n",
    "print(\"Our initial root MSE = {0:.4}\".format(firstRootMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a baseline model and test metric, we can start joining more predictors to see their influence on the model. Then we can try making some interaction terms. Remember that interaction terms may increase the overall **predictive power** of the model, but we will be sacrificing **interpretability**. Interaction between variables and  the predictability/interpretability tradeoff is at the crux of [Artificial Nueral Networks](https://en.wikipedia.org/wiki/Artificial_neural_network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you've used SQL before, then this a simple join on the indices of the original data and our train/test sets\n",
    "pd.options.mode.chained_assignment = None  # default = 'warn'\n",
    "\n",
    "# Select more predictors\n",
    "newPredictors = movieData[['duration', 'actor_1_facebook_likes', 'num_voted_users',\n",
    "                           'actor_2_facebook_likes', 'language']]\n",
    "\n",
    "# Join by the index number\n",
    "movieTrain2 = pd.merge(movieTrain, \n",
    "                       newPredictors, \n",
    "                       left_index = True,\n",
    "                       right_index = True,\n",
    "                       how = 'left')\n",
    "\n",
    "movieTest2 = pd.merge(movieTest, \n",
    "                      newPredictors, \n",
    "                      left_index = True,\n",
    "                      right_index = True,\n",
    "                      how = 'left')\n",
    "\n",
    "# Drop rows with missing values in new features\n",
    "movieTrain2.dropna(axis = 0, how = 'any', inplace = True)\n",
    "movieTest2.dropna(axis = 0, how = 'any', inplace  = True)\n",
    "\n",
    "# Encode language, a categorical variable\n",
    "languageCode = LabelEncoder().fit(newPredictors.language.dropna())\n",
    "movieTrain2['language'] = languageCode.transform(movieTrain2['language'])\n",
    "movieTest2['language'] = languageCode.transform(movieTest2['language'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old root MSE = 1.11.\n",
      "Our improved root MSE = 0.963.\n",
      "An improvement of 13.46.%\n"
     ]
    }
   ],
   "source": [
    "# Let's rebuild and test a model!\n",
    "\n",
    "model2 = LinearRegression().fit(X = movieTrain2.ix[:, movieTrain2.columns != 'imdb_score'],\n",
    "                              y = movieTrain2['imdb_score'])\n",
    "\n",
    "predictions2 = pd.DataFrame(model2.predict(movieTest2.ix[:, movieTest2.columns != 'imdb_score']), \n",
    "                            columns = ['prediction'])\n",
    "\n",
    "MSE2 = metrics.mean_squared_error(y_true = movieTest2.imdb_score,\n",
    "                         y_pred = predictions2)\n",
    "\n",
    "rMSE2 = np.sqrt(MSE2)\n",
    "\n",
    "print('Old root MSE = {0:.3}.'.format(firstRootMSE))\n",
    "print('Our improved root MSE = {0:.3}.'.format(rMSE2))\n",
    "print('An improvement of {0:.4}.%'.format( abs((rMSE2-firstRootMSE)/firstRootMSE)*100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn!\n",
    "\n",
    "Take the wheel, sucker!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
