{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science at UCSB\n",
    "\n",
    "# Python for Data Science: Feature Engineering\n",
    "\n",
    "## Jason Freeberg, Fall 2016\n",
    "\n",
    "Ahoy! Now that we have a basic understanding of machine learning, today we'll go over feature engineering, or, *the process of adding predictors to strengthen the model of a machine learning pipeline*. Let's say a company is building a model to predict trends in the stock market. They will likely begin with historical data on the NASDAQ, Dow Jones, and major stock market leaders like Alphabet Inc., Ford, large oil companies, and banks.\n",
    "\n",
    "Then, as the team attempts to strengthen the predictability of the model, they could choose to include the Consumer Price Index (CPA), national trends in weather, or even a rolling [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) of the Wall Street Journal. These are now new columns, predictors, or *features*, in our dataframe to help our model predict the stock market.\n",
    "\n",
    "Depending on who you ask [interaction terms](https://en.wikipedia.org/wiki/Interaction_(statistics), [scaling](scaling predictors), [centering](http://www.theanalysisfactor.com/center-on-the-mean/), and [standarizing](https://en.wikipedia.org/wiki/Standard_score) could also fall under feature engineering. \n",
    "\n",
    "Since the general idea here is simple, we will spend most of today working through the nitty gritty of adding predictors to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data\n",
    "\n",
    "I wanted to use a time-series dataset to continue the example from earlier, but the the support for time series modeling in Python is limited. Instead we you guts will play with a dataset fit for classification: the historical San Francisco crime breakdown from 2003 to 2015...courtesy of [Kaggle](https://www.kaggle.com/c/sf-crime). Then we'll try adding and mutating our predictors. In a production model, a feature engineer might spend weeks building a web scraper, gathering gigabytes of relevant information, and parsing it down to well formated features... But in today's lab we will simply leave out some given predictors, then add them in to assess their significance ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nrow = 5043. Ncol = 28\n",
      "-------------------- Column Names and Examples --------------------\n",
      "color                           Color               \n",
      "director_name                   Sam Raimi           \n",
      "num_critic_for_reviews          392.0               \n",
      "duration                        156.0               \n",
      "director_facebook_likes         0.0                 \n",
      "actor_3_facebook_likes          4000.0              \n",
      "actor_2_name                    James Franco        \n",
      "actor_1_facebook_likes          24000.0             \n",
      "gross                           336530303.0         \n",
      "genres                          Action|Adventure|Romance\n",
      "actor_1_name                    J.K. Simmons        \n",
      "movie_title                     Spider-Man 3Â        \n",
      "num_voted_users                 383056              \n",
      "cast_total_facebook_likes       46055               \n",
      "actor_3_name                    Kirsten Dunst       \n",
      "facenumber_in_poster            0.0                 \n",
      "plot_keywords                   sandman|spider man|symbiote|venom|villain\n",
      "movie_imdb_link                 http://www.imdb.com/title/tt0413300/?ref_=fn_tt_tt_1\n",
      "num_user_for_reviews            1902.0              \n",
      "language                        English             \n",
      "country                         USA                 \n",
      "content_rating                  PG-13               \n",
      "budget                          258000000.0         \n",
      "title_year                      2007.0              \n",
      "actor_2_facebook_likes          11000.0             \n",
      "imdb_score                      6.2                 \n",
      "aspect_ratio                    2.35                \n",
      "movie_facebook_likes            0                   \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import cross_validation, metrics\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "\n",
    "seed = 123\n",
    "\n",
    "# I'll let you guys play with the San Francisco dataset, I'll use this set from IMDB. \n",
    "# Let's try to predict the film's score from some of the predictors.\n",
    "\n",
    "location = os.path.realpath(os.path.join(os.getcwd(), \"movie_metadata.csv\"))\n",
    "movieData = pd.read_csv(location)\n",
    "\n",
    "print('Nrow = {0}. Ncol = {1}'.format(movieData.shape[0], movieData.shape[1]))\n",
    "print('-------------------- Column Names and Examples --------------------')\n",
    "for col in range(movieData.shape[1]):\n",
    "    print('{0:30}  {1:20}'.format(str(movieData.columns[col]), str(movieData.ix[6, col])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb_score                   float64\n",
      "color                         object\n",
      "title_year                   float64\n",
      "cast_total_facebook_likes      int64\n",
      "budget                       float64\n",
      "dtype: object\n",
      "------- Missing Data: -------\n",
      "imdb_score                     0\n",
      "color                         19\n",
      "title_year                   108\n",
      "cast_total_facebook_likes      0\n",
      "budget                       492\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "startingPreds = movieData[['imdb_score', 'color', 'title_year', 'cast_total_facebook_likes', 'budget']].copy()\n",
    "\n",
    "# Let's print our starting predictors\n",
    "\n",
    "print(startingPreds.dtypes)\n",
    "print('------- Missing Data: -------')\n",
    "print(startingPreds.isnull().sum())\n",
    "\n",
    "# We could spend a whole tutorial on ways to deal with missing data. But for the sake of time \n",
    "# we'll just drop rows with missing predictors.\n",
    "\n",
    "startingPreds.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# Encode our categorical variable\n",
    "\n",
    "colorCode = LabelEncoder().fit(startingPreds['color'])\n",
    "startingPreds['color'] = colorCode.transform(startingPreds['color'])\n",
    "\n",
    "# Model building\n",
    "\n",
    "movieTrain, movieTest = cross_validation.train_test_split(startingPreds, \n",
    "                                                          test_size = 0.3, \n",
    "                                                          random_state = seed)\n",
    "\n",
    "linearModel = LinearRegression().fit(X = movieTrain.ix[:, movieTrain.columns != 'imdb_score'], \n",
    "                                     y = movieTrain['imdb_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our initial root MSE = 1.112\n"
     ]
    }
   ],
   "source": [
    "# Test and evaluate model\n",
    "\n",
    "moviePredictions = pd.DataFrame(linearModel.predict(movieTest.ix[:, movieTest.columns != 'imdb_score']),\n",
    "                                columns = ['predictions'])\n",
    "firstResults = pd.concat([moviePredictions, movieTest['imdb_score'].reset_index(drop=True)], axis = 1)\n",
    "\n",
    "firstMSE = metrics.mean_squared_error(y_true = firstResults['imdb_score'],\n",
    "                                         y_pred = firstResults['predictions'])\n",
    "\n",
    "firstRootMSE = np.sqrt(firstMSE)\n",
    "\n",
    "print(\"Our initial root MSE = {0:.4}\".format(firstRootMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a baseline model and test metric, we can start joining more predictors to see their influence on the model. Then we can try making some interaction terms. Remember that interaction terms may increase the overall **predictive power** of the model, but we will be sacrificing **interpretability**. Interaction between variables and  the predictability/interpretability tradeoff is at the crux of [Artificial Nueral Networks](https://en.wikipedia.org/wiki/Artificial_neural_network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you've used SQL before, then this a simple join on the indices of the original data and our train/test sets\n",
    "pd.options.mode.chained_assignment = None  # default = 'warn'\n",
    "\n",
    "# Select more predictors\n",
    "newPredictors = movieData[['duration', 'actor_1_facebook_likes', 'num_voted_users',\n",
    "                           'actor_2_facebook_likes', 'language']]\n",
    "\n",
    "# Join by the index number\n",
    "movieTrain2 = pd.merge(movieTrain, \n",
    "                       newPredictors, \n",
    "                       left_index = True,\n",
    "                       right_index = True,\n",
    "                       how = 'left')\n",
    "\n",
    "movieTest2 = pd.merge(movieTest, \n",
    "                      newPredictors, \n",
    "                      left_index = True,\n",
    "                      right_index = True,\n",
    "                      how = 'left')\n",
    "\n",
    "# Drop rows with missing values in new features\n",
    "movieTrain2.dropna(axis = 0, how = 'any', inplace = True)\n",
    "movieTest2.dropna(axis = 0, how = 'any', inplace  = True)\n",
    "\n",
    "# Encode language, a categorical variable\n",
    "languageCode = LabelEncoder().fit(newPredictors.language.dropna())\n",
    "movieTrain2['language'] = languageCode.transform(movieTrain2['language'])\n",
    "movieTest2['language'] = languageCode.transform(movieTest2['language'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old root MSE = 1.11.\n",
      "Our improved root MSE = 0.963.\n",
      "An improvement of 13.46%.\n"
     ]
    }
   ],
   "source": [
    "# Let's rebuild and test a model!\n",
    "\n",
    "model2 = LinearRegression().fit(X = movieTrain2.ix[:, movieTrain2.columns != 'imdb_score'],\n",
    "                              y = movieTrain2['imdb_score'])\n",
    "\n",
    "predictions2 = pd.DataFrame(model2.predict(movieTest2.ix[:, movieTest2.columns != 'imdb_score']), \n",
    "                            columns = ['prediction'])\n",
    "\n",
    "MSE2 = metrics.mean_squared_error(y_true = movieTest2.imdb_score,\n",
    "                         y_pred = predictions2)\n",
    "rMSE2 = np.sqrt(MSE2)\n",
    "\n",
    "print('Old root MSE = {0:.3}.'.format(firstRootMSE))\n",
    "print('Our improved root MSE = {0:.3}.'.format(rMSE2))\n",
    "print('An improvement of {0:.4}%.'.format( abs((rMSE2-firstRootMSE)/firstRootMSE)*100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our first root MSE = 1.11.\n",
      "Our second root MSE = 0.963.\n",
      "Our last root MSE = 0.958\n",
      "An improvement of 0.4346%.\n"
     ]
    }
   ],
   "source": [
    "# Now let's add some interaction terms.\n",
    "\n",
    "movieTrain2['budgetXduration'] = movieTrain2.duration * movieTrain2.budget\n",
    "movieTrain2['actor1Xactor2'] = movieTrain2.actor_1_facebook_likes * movieTrain2.actor_2_facebook_likes\n",
    "movieTrain2['castLikesXbudget'] = movieTrain2.cast_total_facebook_likes * movieTrain2.budget\n",
    "\n",
    "movieTest2['budgetXduration'] = movieTest2.duration * movieTest2.budget\n",
    "movieTest2['actor1Xactor2'] = movieTest2.actor_1_facebook_likes * movieTest2.actor_2_facebook_likes\n",
    "movieTest2['castLikesXbudget'] = movieTest2.cast_total_facebook_likes * movieTest2.budget\n",
    "\n",
    "model3 = LinearRegression().fit(X = movieTrain2.ix[:, movieTrain2.columns != 'imdb_score'],\n",
    "                                y = movieTrain2['imdb_score'])\n",
    "\n",
    "predictions3 = pd.DataFrame(model3.predict(X = movieTest2.ix[:, movieTest2.columns != 'imdb_score']), \n",
    "                            columns = ['prediction'])\n",
    "\n",
    "MSE3 = metrics.mean_squared_error(y_true = movieTest2.imdb_score,\n",
    "                                  y_pred = predictions3)\n",
    "rMSE3 = np.sqrt(MSE3)\n",
    "\n",
    "print('Our first root MSE = {0:.3}.'.format(firstRootMSE))\n",
    "print('Our second root MSE = {0:.3}.'.format(rMSE2))\n",
    "print('Our last root MSE = {0:.3}'.format(rMSE3))\n",
    "print('An improvement of {0:.4}%.'.format( abs((rMSE3-rMSE2)/rMSE2)*100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn!\n",
    "\n",
    "Your task is to optimize a classifier that predicts the *type* of crime given the location, time, date, and police district of the crime.\n",
    "\n",
    "Slowly add predictors to improve the model's accuracy, then add interaction terms. We will use the raw accuracy (# correct / # incorrect) to evaluate the model each time. However, there are [other](http://stats.stackexchange.com/questions/51296/how-to-calculate-precision-and-recall-for-multiclass-classification-using-confus) multiclass classification metrics that are worth noting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Rows, Columns) = (676127, 9)\n"
     ]
    }
   ],
   "source": [
    "location1 = os.path.realpath(os.path.join(os.getcwd(), \"crimeTrain.csv\"))\n",
    "crimeData = pd.read_csv(location1)\n",
    "print('(Rows, Columns) =', crimeData.shape)\n",
    "crimeData.head()\n",
    "\n",
    "crimeData = crimeData[['Dates', 'Category', 'PdDistrict', 'Address', 'X', 'Y']]\n",
    "crimeData.to_csv(os.getcwd()+'/crimeData.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Rows, Columns) = (676127, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Category</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-05-13 23:33:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>VANNESS AV / GREENWICH ST</td>\n",
       "      <td>-122.424363</td>\n",
       "      <td>37.800414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-13 23:30:00</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>1500 Block of LOMBARD ST</td>\n",
       "      <td>-122.426995</td>\n",
       "      <td>37.800873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-05-13 23:30:00</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>PARK</td>\n",
       "      <td>100 Block of BRODERICK ST</td>\n",
       "      <td>-122.438738</td>\n",
       "      <td>37.771541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Dates        Category PdDistrict                    Address  \\\n",
       "0  2015-05-13 23:53:00        WARRANTS   NORTHERN         OAK ST / LAGUNA ST   \n",
       "1  2015-05-13 23:53:00  OTHER OFFENSES   NORTHERN         OAK ST / LAGUNA ST   \n",
       "2  2015-05-13 23:33:00  OTHER OFFENSES   NORTHERN  VANNESS AV / GREENWICH ST   \n",
       "3  2015-05-13 23:30:00   LARCENY/THEFT   NORTHERN   1500 Block of LOMBARD ST   \n",
       "4  2015-05-13 23:30:00   LARCENY/THEFT       PARK  100 Block of BRODERICK ST   \n",
       "\n",
       "            X          Y  \n",
       "0 -122.425892  37.774599  \n",
       "1 -122.425892  37.774599  \n",
       "2 -122.424363  37.800414  \n",
       "3 -122.426995  37.800873  \n",
       "4 -122.438738  37.771541  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location1 = os.path.realpath(os.path.join(os.getcwd(), \"crimeData.csv\"))\n",
    "crimeData = pd.read_csv(location1)\n",
    "print('(Rows, Columns) =', crimeData.shape)\n",
    "crimeData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Accuracy = 0.2558\n"
     ]
    }
   ],
   "source": [
    "# If the model fit is running slowly, you can down sample the dataframe here\n",
    "# crimeData = crimeData.sample(frac = 0.6, random_state = seed)\n",
    "\n",
    "# Split up the data into train and test sets\n",
    "crimeTrain, crimeTest = cross_validation.train_test_split(crimeData,\n",
    "                                                         test_size = 0.3,\n",
    "                                                         random_state = seed)\n",
    "# Fill this list with the predictor column names\n",
    "predColumns = ['PdDistrict', 'X', 'Y']\n",
    "\n",
    "# Encode police district, which is a categorical variable\n",
    "districtCode = LabelEncoder().fit(crimeData['PdDistrict'])\n",
    "crimeTrain['PdDistrict'] = districtCode.transform(crimeTrain['PdDistrict'])\n",
    "crimeTest['PdDistrict'] = districtCode.transform(crimeTest['PdDistrict'])\n",
    "\n",
    "# Fit a simple classifier\n",
    "kNN = KNeighborsClassifier(n_neighbors = 10).fit(X = crimeTrain[predColumns],\n",
    "                                                 y = crimeTrain.Category)\n",
    "\n",
    "# Predict on test set\n",
    "crimePredictions1 = pd.DataFrame(kNN.predict(X = crimeTest[predColumns]))\n",
    "accuracy1 = metrics.accuracy_score(y_pred = crimePredictions1,\n",
    "                             y_true = crimeTest.Category)\n",
    "\n",
    "print('Raw Accuracy = {0:.4}'.format(accuracy1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a great start, so let's add more predictors. We'll extract the day of week from the *Dates* column. Notice the ** .map( )** function used on the *Dates* column. **.map( )** takes a [lamba function](http://www.diveintopython.net/power_of_introspection/lambda_functions.html) as its argument, and applies that given function to each element in the column. Lambda functions are just like the normal functions that we learned about weeks ago, except they don't have a name and contain only one expression.\n",
    "\n",
    "Since they need to be simple, I used two in a row. One to parse the string for the date and time, and another to extract the day of week from the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced the numner of addresses from 20624 to 1756.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Category</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>HourOfDay</th>\n",
       "      <th>StrippedAddress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>464683</th>\n",
       "      <td>2008-10-03 23:00:00</td>\n",
       "      <td>DRUG/NARCOTIC</td>\n",
       "      <td>3</td>\n",
       "      <td>MISSION ST / 17TH ST</td>\n",
       "      <td>-122.419516</td>\n",
       "      <td>37.763429</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>MISSION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552131</th>\n",
       "      <td>2007-07-09 10:30:00</td>\n",
       "      <td>ASSAULT</td>\n",
       "      <td>9</td>\n",
       "      <td>0 Block of MASON ST</td>\n",
       "      <td>-122.409268</td>\n",
       "      <td>37.783800</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>MASON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147589</th>\n",
       "      <td>2013-05-19 21:20:00</td>\n",
       "      <td>BURGLARY</td>\n",
       "      <td>4</td>\n",
       "      <td>800 Block of GROVE ST</td>\n",
       "      <td>-122.430563</td>\n",
       "      <td>37.776920</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>GROVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388501</th>\n",
       "      <td>2009-11-11 06:30:00</td>\n",
       "      <td>DISORDERLY CONDUCT</td>\n",
       "      <td>3</td>\n",
       "      <td>3400 Block of 18TH ST</td>\n",
       "      <td>-122.420495</td>\n",
       "      <td>37.761822</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>18TH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600943</th>\n",
       "      <td>2006-10-08 13:00:00</td>\n",
       "      <td>NON-CRIMINAL</td>\n",
       "      <td>1</td>\n",
       "      <td>1ST ST / MARKET ST</td>\n",
       "      <td>-122.399152</td>\n",
       "      <td>37.791017</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>1ST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Dates            Category  PdDistrict  \\\n",
       "464683  2008-10-03 23:00:00       DRUG/NARCOTIC           3   \n",
       "552131  2007-07-09 10:30:00             ASSAULT           9   \n",
       "147589  2013-05-19 21:20:00            BURGLARY           4   \n",
       "388501  2009-11-11 06:30:00  DISORDERLY CONDUCT           3   \n",
       "600943  2006-10-08 13:00:00        NON-CRIMINAL           1   \n",
       "\n",
       "                      Address           X          Y  DayOfWeek  HourOfDay  \\\n",
       "464683   MISSION ST / 17TH ST -122.419516  37.763429          4         23   \n",
       "552131    0 Block of MASON ST -122.409268  37.783800          0         10   \n",
       "147589  800 Block of GROVE ST -122.430563  37.776920          6         21   \n",
       "388501  3400 Block of 18TH ST -122.420495  37.761822          2          6   \n",
       "600943     1ST ST / MARKET ST -122.399152  37.791017          6         13   \n",
       "\n",
       "       StrippedAddress  \n",
       "464683         MISSION  \n",
       "552131           MASON  \n",
       "147589           GROVE  \n",
       "388501            18TH  \n",
       "600943             1ST  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not a great start, let's see how we can improve things.\n",
    "\n",
    "from datetime import datetime    # for extracting the day of week from timestamp\n",
    "import re     # for regular expressions\n",
    "\n",
    "def parseAddress(row):\n",
    "    match = re.search(r'\\b[A-Z]+\\b(?=\\s[A-Z]+)|\\b[0-9]+[A-Z]+\\b', row)\n",
    "    if match == None:\n",
    "        return None\n",
    "    else:\n",
    "        return match.group()\n",
    "\n",
    "crimeTrain['DayOfWeek'] = (crimeTrain.Dates\n",
    "                    .map(lambda row: datetime.strptime(row, '%Y-%m-%d %H:%M:%S'))\n",
    "                    .map(lambda row: datetime.weekday(row))\n",
    "                   )\n",
    "\n",
    "crimeTrain['HourOfDay'] = (crimeTrain.Dates\n",
    "                           .map(lambda row: datetime.strptime(row, '%Y-%m-%d %H:%M:%S'))\n",
    "                           .map(lambda row: row.hour)\n",
    "                          )\n",
    "\n",
    "# Now we'll parse the address and extract street names. We can't use the raw address strings\n",
    "# because there are too many unique strings and it would introduce a lot of noise.\n",
    "\n",
    "crimeTrain['StrippedAddress'] = (crimeTrain.Address\n",
    "                                 .map(lambda row: re.sub(r'THE|LA', '', row))\n",
    "                                 .map(lambda row: parseAddress(row))\n",
    "                                )\n",
    "\n",
    "crimeTrain['StrippedAddress'].fillna('Other')\n",
    "\n",
    "N_address = len(crimeTrain.Address.unique())\n",
    "N_stripAddress = len(crimeTrain.StrippedAddress.unique())\n",
    "print('Reduced the numner of addresses from {0} to {1}.'.format(N_address, N_stripAddress))\n",
    "crimeTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now do the same for the test set...\n",
    "\n",
    "crimeTest['DayOfWeek'] = (crimeTest.Dates\n",
    "                          .map(lambda row: datetime.strptime(row, '%Y-%m-%d %H:%M:%S'))\n",
    "                          .map(lambda row: datetime.weekday(row))\n",
    "                         )\n",
    "\n",
    "crimeTest['HourOfDay'] = (crimeTest.Dates\n",
    "                          .map(lambda row: datetime.strptime(row, '%Y-%m-%d %H:%M:%S'))\n",
    "                          .map(lambda row: row.hour)\n",
    "                         )\n",
    "\n",
    "crimeTest['StrippedAddress'] = (crimeTest.Address\n",
    "                                .map(lambda row: re.sub(r'THE|LA', '', row))\n",
    "                                .map(lambda row: parseAddress(row))\n",
    "                               )\n",
    "\n",
    "crimeTest['StrippedAddress'].fillna('Other')\n",
    "\n",
    "assert crimeTest.shape[1] == 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unorderable types: str() > NoneType()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-e7dfd1bdaf44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mallAddresses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrimeTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStrippedAddress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m                     \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrimeTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStrippedAddress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0maddressCode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallAddresses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcrimeTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'StrippedAddress'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddressCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrimeTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'StrippedAddress'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcrimeTest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'StrippedAddress'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddressCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrimetest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'StrippedAddress'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jasonfreeberg/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0m_check_numpy_unicode_bug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jasonfreeberg/anaconda/lib/python3.5/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unorderable types: str() > NoneType()"
     ]
    }
   ],
   "source": [
    "# And let's encode StrippedAddress, a new categorical variable. Remember that there might be \n",
    "# some addresses present the training set, and absent in the test set. Therefore, we will \n",
    "# create a list of the unique levels from both crimeTrain and crimeTest.\n",
    "\n",
    "allAddresses = list(crimeTrain.StrippedAddress.unique()) + \\\n",
    "                    list(crimeTest.StrippedAddress.unique())\n",
    "\n",
    "addressCode = LabelEncoder().fit(allAddresses)\n",
    "crimeTrain['StrippedAddress'] = addressCode.transform(crimeTrain['StrippedAddress'])\n",
    "crimeTest['StrippedAddress'] = addressCode.transform(crimetest['StrippedAddress'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates                0\n",
      "Category             0\n",
      "PdDistrict           0\n",
      "Address              0\n",
      "X                    0\n",
      "Y                    0\n",
      "DayOfWeek            0\n",
      "HourOfDay            0\n",
      "StrippedAddress    346\n",
      "dtype: int64\n",
      "Dates                0\n",
      "Category             0\n",
      "PdDistrict           0\n",
      "Address              0\n",
      "X                    0\n",
      "Y                    0\n",
      "DayOfWeek            0\n",
      "HourOfDay            0\n",
      "StrippedAddress    871\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
