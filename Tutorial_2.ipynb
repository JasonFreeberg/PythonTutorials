{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Science at UCSB\n",
    "# Python for Data Science: Tabular Data\n",
    "**Jason Freeberg, Fall 2016**\n",
    "\n",
    "Tabular data is how people structure a lot of the data you will come across. It is not the *only* data format, but it is the easiest to work with because it is well-structured. Other data formats you will come across include [JSON](http://www.json.org/), [Relational and Non-Relational Databases](https://www.mongodb.com/scale/relational-vs-non-relational-database), images, and audio files. And believe it or not you are already familiar with tabular data, it's simply a table with columns and rows. Just like in Excel.\n",
    "\n",
    "As data scientists to-be, however, we need to make define some terms. We will often refer to rows as *observations* or *records*, and columns as *variables* or *features*. The *header* is the top row containing the names of our variables. In the example below our variables are country, salesperson, order id, and so on. Our observations are individual orders with those variable values. Our header, in this case, would be the row with index #1.\n",
    "\n",
    "![data_pic](http://mothimages.s3.amazonaws.com/tabular_data_1.png)\n",
    "\n",
    "In today's lab we will get acquainted with the [pandas module](http://pandas.pydata.org/) by loading a Comma Seperated Value (.csv) [file](https://archive.ics.uci.edu/ml/datasets/Forest+Fires) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.html). We will then check it, coerce the variables to the correct format, check for missing values, and create aggregate reports by conditional selection. Then you'll follow the same pipeline on your own with a different dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules we'll need and assign the data's URL.\n",
    "\n",
    "# By the way, it's customary to include all module imports at the beginning of your script.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib2\n",
    "\n",
    "UCI_data_URL = 'http://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to read the data from the url. \n",
    "# Just run this cell, but understand what the function is doing.\n",
    "\n",
    "\n",
    "def read_csv_from_url(URL):\n",
    "    \"\"\"\n",
    "    Takes as input a string containing the URL pointing to a dataset from the UCI data repository.\n",
    "    Returns a pandas dataframe containing the data with some columns coerced as strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = urllib2.urlopen(URL)\n",
    "    lines = pd.read_csv(response, \n",
    "                        header = 0,\n",
    "                        index_col = False,\n",
    "                        dtype = {'DMC' : str,\n",
    "                                 'temp' : str,\n",
    "                                 'area' : str})\n",
    "    \n",
    "    return pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Check Data\n",
    "\n",
    "Using the URL and function above, let's load the data into our notebook as a pandas dataframe. We will then inspect dataframe's size, missing values, and variable types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and print the head of the dataframe.\n",
    "\n",
    "fire_df = read_csv_from_url(UCI_data_URL)\n",
    "\n",
    "# Let's check the head and size of our data\n",
    "\n",
    "print fire_df.head()\n",
    "print \"Number of rows:\", fire_df.shape[0]\n",
    "print \"Number of columns:\", fire_df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our data loaded and assigned as a pandas.DataFrame object. However, I made a *slight* adjustment and loaded some variables as **strings**. Know that the pandas DataFrame( ) method is very well built and could have inferred the correct types for all columns, but variable coersion is a common data preparation task so we will do it in this lab.\n",
    "\n",
    "**Right now we'll check for missing and incorrect values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .isnull() returns a dataframe of logical (T/F) entries where True = Is_Null and False = Not_Null.\n",
    "# We can use the sum() method to take the sums by each column. Remember that True = 1, False = 0.\n",
    "\n",
    "logical_dataframe = fire_df.isnull()\n",
    "print logical_dataframe.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we don't have any **NaN** or **None** values in our columns. But we're not out of the woods yet. Let's take a look at our categorical variables and check that they're reasonable. By printing out the unique strings in each column, we'll be able to see if there are any inappropriate values like misspelled days or months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The syntax, \"dataFrame.columnName\" will return a pandas Series object. \n",
    "# We can use the unique() method to get the distinct strings held in the Series object. \n",
    "\n",
    "print \"Class of our returned column:\", type(fire_df.month)\n",
    "print fire_df.month.unique()\n",
    "print fire_df.day.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily for us the UCI datasets are often very clean. Although we didn't uncover any missing or incorrect values in this dataset, these types of checks will become routine when you start a project or intern at a company.\n",
    "\n",
    "**Now we'll look at our column types and make adjustments as necessary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show our columns and their corresponding types.\n",
    "print 'The data types of our features:'\n",
    "print fire_df.dtypes, '\\n'\n",
    "\n",
    "# That's a lot to look at, let's narrow our search. This is a conditional selection, which we'll get to later.\n",
    "print 'Our non-numeric variables:'\n",
    "print fire_df.dtypes[fire_df.dtypes == 'object']  # Condition is in the square brackets.\n",
    "\n",
    "# Month and day are okay being objects (strings), but those other three need to be converted to floats...\n",
    "fire_df.DMC = fire_df.DMC.astype(float)\n",
    "fire_df.area = fire_df.area.astype(float)\n",
    "fire_df.temp = fire_df.temp.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Selection\n",
    "\n",
    "Now that we have vetted the data for discrepancies, we can create do some exploratory analysis. Let's first cover conditional selection. Our data is 517 x 13, but we often won't want to use the entire table all the time. We might only need a couple columns, or perhaps we only want to look at the data on Tuesdays. With pandas, it's easy to select columns and rows based on arbitrary conditions. \n",
    "\n",
    "- Here's the basic syntax: *dataframe*[*condition on **rows***]\\[*names or numbers of **columns***]\n",
    "- Alternatively... *dataframe*.ix[*condition on **rows***, *selection of **columns*** ]\n",
    "  - If you come from an R background, the .ix attribute syntax may seem familiar\n",
    "- use *dataframe*.iloc**[ ]** for selecting rows and columns based on **purely numerical** indices\n",
    "\n",
    "Click [here](http://pandas.pydata.org/pandas-docs/stable/indexing.html) for the full documentation on slicing and dicing pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows...\n",
    "fire_df[:5]  # prints first ten rows (from 0 to 9)\n",
    "fire_df.iloc[:5]  # same as above\n",
    "fire_df.iloc[[1,2,3,4,5]]  # same again\n",
    "fire_df[fire_df.area > 30]\n",
    "fire_df[ (fire_df.area > 30) & (fire_df.rain > 10) ]  # two conditions on rows\n",
    "\n",
    "# Select columns...\n",
    "fire_df.temp  # a single column\n",
    "fire_df[\"temp\"]  # also a single column \n",
    "fire_df[['day', 'area', 'rain']]  # multiple columns\n",
    "\n",
    "# Select AND filter...\n",
    "fire_df[fire_df.area > 30][['day', 'area', 'rain']]\n",
    "\n",
    "# Using '.ix' and making the same selection as above...\n",
    "fire_df.ix[ fire_df.area > 30, ['day', 'area', 'rain'] ]\n",
    "\n",
    "# Just to hide output...\n",
    "print(\"Wow Python is so cool!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn\n",
    "\n",
    "Get in the driver's seat, because it's your turn to write some code. Look for the <FILL IN> bits. Good luck and be sure to ask Jason for clarification or help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}